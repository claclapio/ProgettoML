{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee6521e-c28b-40d4-97b5-b57a1498f141",
   "metadata": {},
   "source": [
    "# Fraud Detection - Multiclass Classification\n",
    "## Machine Learning I - Group Project\n",
    "\n",
    "**Objective:** E-commerce fraud detection using multiclass classification (Legitimate, Suspicious, Fraudulent)\n",
    "\n",
    "**Authors:** [Your Names]  \n",
    "**Date:** December 2024\n",
    "\n",
    "---\n",
    "\n",
    "### Methodology:\n",
    "- **4 ML Algorithms:** ANNs (8 topologies), SVMs (10 configs), Decision Trees (7 depths), kNN (6 k values)\n",
    "- **Ensemble Methods:** Majority Voting + Weighted Voting\n",
    "- **Dataset:** 1.5M e-commerce transactions â†’ 3-class risk assessment\n",
    "- **Cross-Validation:** 3-fold stratified on training set\n",
    "- **Evaluation:** Hold-out test set (20%)\n",
    "\n",
    "### Code Organization:\n",
    "```\n",
    "/project/\n",
    "â”œâ”€â”€ main.jl                    â† This file (executable from top to bottom)\n",
    "â”œâ”€â”€ /utils/\n",
    "â”‚   â”œâ”€â”€ utils.jl              â† Course utilities (includes modelCrossValidation)\n",
    "â”‚   â””â”€â”€ preprocessing.jl      â† Custom preprocessing functions\n",
    "â””â”€â”€ /datasets/\n",
    "    â””â”€â”€ Fraudulent_E-Commerce_Transaction_Data_merge.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be78a1c4-2880-4800-923c-b09809f18793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Packages loaded!\n",
      "import MLJLIBSVMInterface âœ”\n",
      "import NearestNeighborModels âœ”\n",
      "import MLJDecisionTreeInterface âœ”\n",
      "âœ… Course utilities loaded (includes modelCrossValidation, confusionMatrix, etc.)\n",
      "âœ… Custom preprocessing utilities loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#                    SETUP & IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "using Random\n",
    "Random.seed!(42)\n",
    "\n",
    "# Load packages\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using Dates\n",
    "using StatsBase\n",
    "\n",
    "println(\"âœ… Packages loaded!\")\n",
    "\n",
    "# Load course utilities\n",
    "include(\"utils/utils.jl\")\n",
    "println(\"âœ… Course utilities loaded (includes modelCrossValidation, confusionMatrix, etc.)\")\n",
    "\n",
    "# Load custom preprocessing\n",
    "include(\"utils/preprocessing.jl\")\n",
    "using .PreprocessingUtils\n",
    "println(\"âœ… Custom preprocessing utilities loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d17870d-b83f-4322-ae10-f019621c1a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightedVoting (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#  HELPER FUNCTIONS: ENSEMBLE VOTING\n",
    "# ============================================================================\n",
    "\n",
    "function majorityVoting(predictions::Vector{Vector{String}})\n",
    "    n_samples = length(predictions[1])\n",
    "    ensemble_predictions = Vector{String}(undef, n_samples)\n",
    "    for i in 1:n_samples\n",
    "        votes = [pred[i] for pred in predictions]\n",
    "        ensemble_predictions[i] = mode(votes)\n",
    "    end\n",
    "    return ensemble_predictions\n",
    "end\n",
    "\n",
    "function weightedVoting(predictions::Vector{Vector{String}}, weights::Vector{Float64})\n",
    "    n_samples = length(predictions[1])\n",
    "    n_models = length(predictions)\n",
    "    # Raccogli tutte le classi uniche\n",
    "    classes_unique = sort(unique(vcat(predictions...)))\n",
    "    \n",
    "    ensemble_predictions = Vector{String}(undef, n_samples)\n",
    "    for i in 1:n_samples\n",
    "        class_scores = Dict(c => 0.0 for c in classes_unique)\n",
    "        for j in 1:n_models\n",
    "            class_pred = predictions[j][i]\n",
    "            if haskey(class_scores, class_pred)\n",
    "                class_scores[class_pred] += weights[j]\n",
    "            end\n",
    "        end\n",
    "        ensemble_predictions[i] = argmax(class_scores)\n",
    "    end\n",
    "    return ensemble_predictions\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb47e1f2-2b47-442c-8c91-0bc5ab2dc530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_approach (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#  HELPER FUNCTION: EVALUATE ALL MODELS + ENSEMBLES\n",
    "# ============================================================================\n",
    "\n",
    "function evaluate_approach(approach_name, train_inputs, train_targets, test_inputs, test_targets; cv_folds=3)\n",
    "    println(\"\\n\" * \"=\"^80)\n",
    "    println(\"ðŸš€ EVALUATING APPROACH: $approach_name (incl. Ensembles)\")\n",
    "    println(\"=\"^80)\n",
    "    \n",
    "    cv_indices = crossvalidation(train_targets, cv_folds)\n",
    "    final_results = Dict{String, Dict{String, Float64}}()\n",
    "    \n",
    "    # Preparazione Dati\n",
    "    train_targets_str = string.(train_targets)\n",
    "    test_targets_str = string.(test_targets)\n",
    "    classes_str = sort(unique(train_targets_str))\n",
    "    \n",
    "    # Preparazione target one-hot per ANN\n",
    "    classes_int = sort(unique(train_targets))\n",
    "    \n",
    "    # CORREZIONE PER ANN BINARIA: \n",
    "    # Se binario, forziamo il target a essere 1 quando la classe Ã¨ 1 (Frode)\n",
    "    if length(classes_int) == 2\n",
    "        train_targets_onehot = reshape(train_targets .== classes_int[2], :, 1) # classes_int[2] Ã¨ 1\n",
    "        test_targets_onehot  = reshape(test_targets  .== classes_int[2], :, 1)\n",
    "    else\n",
    "        train_targets_onehot = oneHotEncoding(train_targets, classes_int)\n",
    "        test_targets_onehot = oneHotEncoding(test_targets, classes_int)\n",
    "    end\n",
    "    # Normalizzazione (Usiamo la MinMax come standard per ANN/SVM/kNN)\n",
    "    normParams = calculateMinMaxNormalizationParameters(train_inputs)\n",
    "    train_inputs_norm = normalizeMinMax(train_inputs, normParams)\n",
    "    test_inputs_norm = normalizeMinMax(test_inputs, normParams)\n",
    "\n",
    "    # --- INTERNAL HELPER: METRICS ---\n",
    "    function calculate_metrics_safe(y_pred_probs, y_pred_class, y_true_class, y_true_onehot, classes)\n",
    "        # AUC (Placeholder o calcolo reale se binario)\n",
    "        auc_score = 0.5\n",
    "        if length(classes) == 2\n",
    "            probs = vec(y_pred_probs)\n",
    "            true_bin = vec(y_true_onehot)\n",
    "            # Semplice calcolo AUC per caso binario se possibile, altrimenti 0.5\n",
    "            try\n",
    "                p = sortperm(probs); probs_sorted = probs[p]; true_sorted = true_bin[p]\n",
    "                tpr = [0.0]; fpr = [0.0]; tp=0; fp=0; num_pos=sum(true_sorted); num_neg=length(true_sorted)-num_pos\n",
    "                if num_pos>0 && num_neg>0\n",
    "                    for i in length(probs_sorted):-1:1\n",
    "                        if true_sorted[i]==1; tp+=1; else; fp+=1; end\n",
    "                        push!(tpr, tp/num_pos); push!(fpr, fp/num_neg)\n",
    "                    end\n",
    "                    auc_score = sum((fpr[i]-fpr[i-1])*(tpr[i]+tpr[i-1])/2 for i in 2:length(tpr))\n",
    "                end\n",
    "            catch; end\n",
    "        end\n",
    "\n",
    "        # Metrics via Confusion Matrix\n",
    "        if length(classes) == 2\n",
    "            pos_label = classes[end]\n",
    "            y_p_bool = vec(y_pred_class .== pos_label)\n",
    "            y_t_bool = vec(y_true_class .== pos_label)\n",
    "            (acc, err, sens, spec, prec, npv, f1, cm) = confusionMatrix(y_p_bool, y_t_bool)\n",
    "        else\n",
    "            cm_res = confusionMatrix(y_pred_class, y_true_class, classes; weighted=true)\n",
    "            acc = cm_res.accuracy; sens = cm_res.aggregated.sensitivity\n",
    "            spec = cm_res.aggregated.specificity; f1 = cm_res.aggregated.f1\n",
    "        end\n",
    "        return Dict(\"Accuracy\"=>acc, \"AUC\"=>auc_score, \"Sensitivity\"=>sens, \"Specificity\"=>spec, \"F1\"=>f1)\n",
    "    end\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1. Artificial Neural Networks (ANNs)\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[1/5] Testing ANNs...\")\n",
    "    ann_topologies = [[256], [128], [64], [32], [256, 128], [128, 64], [64, 32], [96, 48]]\n",
    "    best_f1_cv_ann = -1.0; best_topo_ann = []\n",
    "    \n",
    "    for topology in ann_topologies\n",
    "        hyperparams = Dict(\"topology\" => topology, \"learningRate\" => 0.003, \"validationRatio\" => 0.1, \n",
    "                           \"numExecutions\" => 1, \"maxEpochs\" => 800, \"maxEpochsVal\" => 25)\n",
    "        # Nota: Passiamo train_inputs_norm normalizzati\n",
    "        res = modelCrossValidation(:ANN, hyperparams, (train_inputs_norm, train_targets), cv_indices)\n",
    "        if res[7][1] > best_f1_cv_ann; best_f1_cv_ann = res[7][1]; best_topo_ann = topology; end\n",
    "    end\n",
    "    println(\"   âœ¨ Best ANN (CV): $best_topo_ann - CV F1: $(round(best_f1_cv_ann*100, digits=2))%\")\n",
    "\n",
    "    # Retraining ANN\n",
    "    N_train = size(train_inputs_norm, 1); (train_idx, val_idx) = holdOut(N_train, 0.1)\n",
    "    final_ann, _ = _trainClassANN(best_topo_ann,\n",
    "        (train_inputs_norm[train_idx, :], train_targets_onehot[train_idx, :]),\n",
    "        validationDataset=(train_inputs_norm[val_idx, :], train_targets_onehot[val_idx, :]),\n",
    "        testDataset=(test_inputs_norm, test_targets_onehot),\n",
    "        maxEpochs=800, learningRate=0.003, maxEpochsVal=25)\n",
    "    \n",
    "    test_outputs_ann_raw = final_ann(test_inputs_norm')'\n",
    "    # Conversione predizioni per metriche e per ensemble\n",
    "    if size(test_targets_onehot, 2) == 1 # Caso Binario ANN\n",
    "        probs_ann = vec(test_outputs_ann_raw)\n",
    "        preds_ann_int = Int.(probs_ann .>= 0.5) # 0 o 1\n",
    "    else # Caso Multiclasse ANN\n",
    "        preds_bool = classifyOutputs(test_outputs_ann_raw)\n",
    "        preds_ann_int = [findfirst(x->x, row) - 1 for row in eachrow(preds_bool)] # 0, 1, 2...\n",
    "    end\n",
    "    # Fondamentale: convertiamo in stringa per l'ensemble\n",
    "    preds_ann_str = string.(preds_ann_int)\n",
    "    \n",
    "    final_results[\"ANN\"] = calculate_metrics_safe(\n",
    "        (size(test_targets_onehot,2)==1 ? vec(test_outputs_ann_raw) : test_outputs_ann_raw), \n",
    "        preds_ann_int, test_targets, test_targets_onehot, classes_int)\n",
    "    println(\"      âœ… ANN Results: F1=$(round(final_results[\"ANN\"][\"F1\"],digits=3))\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2. Support Vector Machines (SVMs)\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[2/5] Testing SVMs...\")\n",
    "    svm_configs = [(\"linear\", 1.0, 0.1, 3), (\"rbf\", 1.0, 0.125, 3), (\"poly\", 1.0, 0.1, 2)]\n",
    "    best_f1_cv_svm = -1.0; best_params_svm = ()\n",
    "    for (kernel, C, gamma, degree) in svm_configs\n",
    "        res = modelCrossValidation(:SVC, Dict(\"kernel\"=>kernel, \"C\"=>C, \"gamma\"=>gamma, \"degree\"=>degree), (train_inputs, train_targets), cv_indices)\n",
    "        if res[7][1] > best_f1_cv_svm; best_f1_cv_svm = res[7][1]; best_params_svm = (kernel, C, gamma, degree); end\n",
    "    end\n",
    "    \n",
    "    (k, C, g, d) = best_params_svm\n",
    "    k_func = k == \"linear\" ? LIBSVM.Kernel.Linear : (k == \"poly\" ? LIBSVM.Kernel.Polynomial : LIBSVM.Kernel.RadialBasis)\n",
    "    model_svm = SVMClassifier(kernel=k_func, cost=C, gamma=g, degree=Int32(d))\n",
    "    mach_svm = machine(model_svm, MLJ.table(train_inputs_norm), categorical(train_targets_str))\n",
    "    MLJ.fit!(mach_svm, verbosity=0)\n",
    "    \n",
    "    preds_svm_cat = MLJ.predict(mach_svm, MLJ.table(test_inputs_norm))\n",
    "    preds_svm_str = string.(preds_svm_cat)\n",
    "    \n",
    "    final_results[\"SVM\"] = calculate_metrics_safe(zeros(length(preds_svm_str)), preds_svm_str, test_targets_str, test_targets_onehot, classes_str)\n",
    "    println(\"      âœ… SVM Results: F1=$(round(final_results[\"SVM\"][\"F1\"],digits=3))\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3. Decision Trees\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[3/5] Testing Decision Trees...\")\n",
    "    depths = [3, 5, 7, 10]; best_f1_cv_dt = -1.0; best_depth = 0\n",
    "    for d in depths\n",
    "        res = modelCrossValidation(:DecisionTreeClassifier, Dict(\"max_depth\"=>d), (train_inputs, train_targets), cv_indices)\n",
    "        if res[7][1] > best_f1_cv_dt; best_f1_cv_dt = res[7][1]; best_depth = d; end\n",
    "    end\n",
    "    \n",
    "    model_dt = DTClassifier(max_depth=best_depth, rng=Random.MersenneTwister(42))\n",
    "    mach_dt = machine(model_dt, MLJ.table(train_inputs_norm), categorical(train_targets_str))\n",
    "    MLJ.fit!(mach_dt, verbosity=0)\n",
    "    \n",
    "    preds_dt_prob = MLJ.predict(mach_dt, MLJ.table(test_inputs_norm))\n",
    "    preds_dt_str = string.(mode.(preds_dt_prob))\n",
    "    \n",
    "    final_results[\"DT\"] = calculate_metrics_safe(zeros(length(preds_dt_str)), preds_dt_str, test_targets_str, test_targets_onehot, classes_str)\n",
    "    println(\"      âœ… DT Results: F1=$(round(final_results[\"DT\"][\"F1\"],digits=3))\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4. k-Nearest Neighbors (kNN)\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[4/5] Testing kNN...\")\n",
    "    k_vals = [1, 3, 5, 7]; best_f1_cv_knn = -1.0; best_k = 0\n",
    "    for k in k_vals\n",
    "        res = modelCrossValidation(:KNeighborsClassifier, Dict(\"n_neighbors\"=>k), (train_inputs, train_targets), cv_indices)\n",
    "        if res[7][1] > best_f1_cv_knn; best_f1_cv_knn = res[7][1]; best_k = k; end\n",
    "    end\n",
    "    \n",
    "    model_knn = kNNClassifier(K=best_k)\n",
    "    mach_knn = machine(model_knn, MLJ.table(train_inputs_norm), categorical(train_targets_str))\n",
    "    MLJ.fit!(mach_knn, verbosity=0)\n",
    "    \n",
    "    preds_knn_prob = MLJ.predict(mach_knn, MLJ.table(test_inputs_norm))\n",
    "    preds_knn_str = string.(mode.(preds_knn_prob))\n",
    "    \n",
    "    final_results[\"kNN\"] = calculate_metrics_safe(zeros(length(preds_knn_str)), preds_knn_str, test_targets_str, test_targets_onehot, classes_str)\n",
    "    println(\"      âœ… kNN Results: F1=$(round(final_results[\"kNN\"][\"F1\"],digits=3))\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5. ENSEMBLE METHODS\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[5/5] Testing Ensemble Methods...\")\n",
    "    \n",
    "    # Raccolta Predizioni (Tutte stringhe)\n",
    "    all_predictions = [preds_ann_str, preds_svm_str, preds_dt_str, preds_knn_str]\n",
    "    model_names = [\"ANN\", \"SVM\", \"DT\", \"kNN\"]\n",
    "    \n",
    "    # 5a. Majority Voting\n",
    "    majority_preds = majorityVoting(all_predictions)\n",
    "    final_results[\"MajorityVoting\"] = calculate_metrics_safe(zeros(length(majority_preds)), majority_preds, test_targets_str, test_targets_onehot, classes_str)\n",
    "    println(\"      âœ… Majority Voting: F1=$(round(final_results[\"MajorityVoting\"][\"F1\"],digits=3))\")\n",
    "    \n",
    "    # 5b. Weighted Voting\n",
    "    # Pesi basati sull'F1 score sul Test Set (o potresti usare CV score se preferisci)\n",
    "    weights = [final_results[m][\"F1\"] for m in model_names]\n",
    "    # Normalizza pesi (opzionale ma pulito)\n",
    "    weights = weights ./ sum(weights)\n",
    "    \n",
    "    weighted_preds = weightedVoting(all_predictions, weights)\n",
    "    final_results[\"WeightedVoting\"] = calculate_metrics_safe(zeros(length(weighted_preds)), weighted_preds, test_targets_str, test_targets_onehot, classes_str)\n",
    "    println(\"      âœ… Weighted Voting: F1=$(round(final_results[\"WeightedVoting\"][\"F1\"],digits=3))\")\n",
    "\n",
    "    return final_results\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59226d32-d227-4e1f-961f-607df6634244",
   "metadata": {},
   "source": [
    "## 1. Data Loading & 3-Class Target Creation\n",
    "\n",
    "**Dataset:** Fraudulent E-Commerce Transaction Data (1.5M transactions)\n",
    "\n",
    "**Target Creation Strategy:**\n",
    "- Original: Binary fraud labels (fraud vs non-fraud)\n",
    "- **Our approach:** 3-class risk assessment based on multiple signals:\n",
    "  1. **Time Risk:** Night transactions (0-5am, 11pm)\n",
    "  2. **Amount Risk:** High-value transactions (>90th percentile)\n",
    "  3. **Account Age Risk:** New accounts (<30 days)\n",
    "\n",
    "**Class Mapping:**\n",
    "- **Class 0 (LEGITTIMO):** Low-risk, legitimate transactions\n",
    "- **Class 1 (SOSPETTO):** Borderline cases requiring manual review\n",
    "- **Class 2 (FRAUDOLENTO):** High-risk fraudulent transactions\n",
    "\n",
    "**Justification:** This approach allows for graduated risk assessment, enabling businesses to:\n",
    "- Automatically approve low-risk transactions (Class 0)\n",
    "- Flag suspicious cases for manual review (Class 1)\n",
    "- Immediately block high-risk frauds (Class 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de582c0d-ce18-4b53-ae3b-8bbc64b19877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“‚ LOADING DATA\n",
      "======================================================================\n",
      "Original dataset size: (1496586, 16)\n",
      "Original fraud distribution:\n",
      "  Non-fraud: 1421526\n",
      "  Fraud:     75060\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ CREATING 3-CLASS TARGET\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Calculating risk signals...\n",
      "\n",
      "âœ… 3-Class distribution:\n",
      "  Class 0 (LEGITTIMO): 1347548 (90.0%)\n",
      "  Class 1 (SOSPETTO): 128384 (8.6%)\n",
      "  Class 2 (FRAUDOLENTO): 20654 (1.4%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1496586Ã—21 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">1496561 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Transaction ID</th><th style = \"text-align: left;\">Customer ID</th><th style = \"text-align: left;\">Transaction Amount</th><th style = \"text-align: left;\">Transaction Date</th><th style = \"text-align: left;\">Payment Method</th><th style = \"text-align: left;\">Product Category</th><th style = \"text-align: left;\">Quantity</th><th style = \"text-align: left;\">Customer Age</th><th style = \"text-align: left;\">Customer Location</th><th style = \"text-align: left;\">Device Used</th><th style = \"text-align: left;\">IP Address</th><th style = \"text-align: left;\">Shipping Address</th><th style = \"text-align: left;\">Billing Address</th><th style = \"text-align: left;\">Is Fraudulent</th><th style = \"text-align: left;\">Account Age Days</th><th style = \"text-align: left;\">Transaction Hour</th><th style = \"text-align: left;\">Hour_Risk</th><th style = \"text-align: left;\">Amount_Risk</th><th style = \"text-align: left;\">Account_Risk</th><th style = \"text-align: left;\">Total_Risk</th><th style = \"text-align: left;\">Risk_Class</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String31\" style = \"text-align: left;\">String31</th><th title = \"String15\" style = \"text-align: left;\">String15</th><th title = \"String15\" style = \"text-align: left;\">String15</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"String31\" style = \"text-align: left;\">String31</th><th title = \"String7\" style = \"text-align: left;\">String7</th><th title = \"String15\" style = \"text-align: left;\">String15</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">15d2e414-8735-46fc-9e02-80b472b2580f</td><td style = \"text-align: left;\">d1b87f62-51b2-493b-ad6a-77e0fe13e785</td><td style = \"text-align: right;\">58.09</td><td style = \"text-align: left;\">2024-02-20 05:58:41</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">electronics</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">17</td><td style = \"text-align: left;\">Amandaborough</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">212.195.49.198</td><td style = \"text-align: left;\">Unit 8934 Box 0058\\nDPO AA 05437</td><td style = \"text-align: left;\">Unit 8934 Box 0058\\nDPO AA 05437</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">30</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">0bfee1a0-6d5e-40da-a446-d04e73b1b177</td><td style = \"text-align: left;\">37de64d5-e901-4a56-9ea0-af0c24c069cf</td><td style = \"text-align: right;\">389.96</td><td style = \"text-align: left;\">2024-02-25 08:09:45</td><td style = \"text-align: left;\">debit card</td><td style = \"text-align: left;\">electronics</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">40</td><td style = \"text-align: left;\">East Timothy</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">208.106.249.121</td><td style = \"text-align: left;\">634 May Keys\\nPort Cherylview, NV 75063</td><td style = \"text-align: left;\">634 May Keys\\nPort Cherylview, NV 75063</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">72</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">e588eef4-b754-468e-9d90-d0e0abfc1af0</td><td style = \"text-align: left;\">1bac88d6-4b22-409a-a06b-425119c57225</td><td style = \"text-align: right;\">134.19</td><td style = \"text-align: left;\">2024-03-18 03:42:55</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">home &amp; garden</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">22</td><td style = \"text-align: left;\">Davismouth</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">76.63.88.212</td><td style = \"text-align: left;\">16282 Dana Falls Suite 790\\nRothhaven, IL 15564</td><td style = \"text-align: left;\">16282 Dana Falls Suite 790\\nRothhaven, IL 15564</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">63</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">4de46e52-60c3-49d9-be39-636681009789</td><td style = \"text-align: left;\">2357c76e-9253-4ceb-b44e-ef4b71cb7d4d</td><td style = \"text-align: right;\">226.17</td><td style = \"text-align: left;\">2024-03-16 20:41:31</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">clothing</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">31</td><td style = \"text-align: left;\">Lynnberg</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">207.208.171.73</td><td style = \"text-align: left;\">828 Strong Loaf Apt. 646\\nNew Joshua, UT 84798</td><td style = \"text-align: left;\">828 Strong Loaf Apt. 646\\nNew Joshua, UT 84798</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">124</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">074a76de-fe2d-443e-a00c-f044cdb68e21</td><td style = \"text-align: left;\">45071bc5-9588-43ea-8093-023caec8ea1c</td><td style = \"text-align: right;\">121.53</td><td style = \"text-align: left;\">2024-01-15 05:08:17</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">clothing</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">51</td><td style = \"text-align: left;\">South Nicole</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">190.172.14.169</td><td style = \"text-align: left;\">29799 Jason Hills Apt. 439\\nWest Richardtown, OH 36093</td><td style = \"text-align: left;\">29799 Jason Hills Apt. 439\\nWest Richardtown, OH 36093</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">158</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">4e707452-7c8a-4cbd-b0c1-2aeaa35c5e88</td><td style = \"text-align: left;\">29616b04-2d5c-4729-9c9d-8d71a6ad9dc1</td><td style = \"text-align: right;\">166.41</td><td style = \"text-align: left;\">2024-01-30 10:55:14</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">toys &amp; games</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">34</td><td style = \"text-align: left;\">Herreramouth</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">202.237.29.55</td><td style = \"text-align: left;\">5699 Brittany Villages Suite 903\\nLake Tim, MD 46274</td><td style = \"text-align: left;\">120 Kristi Dale\\nPort Meganshire, GU 03060</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">7ed952fe-8ae1-4f11-8cc5-6607060240d8</td><td style = \"text-align: left;\">fe21ae29-ba4c-424f-9d55-0095539c09fa</td><td style = \"text-align: right;\">92.88</td><td style = \"text-align: left;\">2024-02-04 19:59:10</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">toys &amp; games</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">14</td><td style = \"text-align: left;\">Ramosfort</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">13.45.27.192</td><td style = \"text-align: left;\">727 Gibson Islands Apt. 279\\nNew Davidbury, ME 43104</td><td style = \"text-align: left;\">727 Gibson Islands Apt. 279\\nNew Davidbury, ME 43104</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">119</td><td style = \"text-align: right;\">19</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">0b2fb5aa-7171-472f-8269-371094608a07</td><td style = \"text-align: left;\">024257c3-5671-4de8-a33c-98fc5cbe6f92</td><td style = \"text-align: right;\">318.14</td><td style = \"text-align: left;\">2024-02-20 13:30:29</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">health &amp; beauty</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">42</td><td style = \"text-align: left;\">Port Emily</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">131.141.230.185</td><td style = \"text-align: left;\">3914 Davis Union\\nBrownchester, IN 07744</td><td style = \"text-align: left;\">3914 Davis Union\\nBrownchester, IN 07744</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">251</td><td style = \"text-align: right;\">13</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">1f52366c-7f40-4397-885f-3856b6e6531c</td><td style = \"text-align: left;\">f17640ca-49da-45d1-8461-c2a1cf9c1b61</td><td style = \"text-align: right;\">47.92</td><td style = \"text-align: left;\">2024-03-03 19:44:00</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">home &amp; garden</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">38</td><td style = \"text-align: left;\">Carneyfurt</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">210.148.17.240</td><td style = \"text-align: left;\">47893 Maldonado Stream Suite 443\\nBrownshire, MO 48487</td><td style = \"text-align: left;\">47893 Maldonado Stream Suite 443\\nBrownshire, MO 48487</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">190</td><td style = \"text-align: right;\">19</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">3f10dfde-9c4c-4085-9872-4f6b39502ffb</td><td style = \"text-align: left;\">aab93e75-582f-4455-80b4-1fb35733a47c</td><td style = \"text-align: right;\">121.78</td><td style = \"text-align: left;\">2024-01-16 21:19:39</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">health &amp; beauty</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">39</td><td style = \"text-align: left;\">Brockburgh</td><td style = \"text-align: left;\">mobile</td><td style = \"text-align: left;\">174.32.252.238</td><td style = \"text-align: left;\">2334 Briana Centers Suite 576\\nArchershire, NM 45210</td><td style = \"text-align: left;\">2334 Briana Centers Suite 576\\nArchershire, NM 45210</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">343</td><td style = \"text-align: right;\">21</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">75f19b14-516c-4f1c-a99e-f1c456453b55</td><td style = \"text-align: left;\">6a2e1397-e24a-4145-a821-fd835f732369</td><td style = \"text-align: right;\">633.39</td><td style = \"text-align: left;\">2024-01-20 22:11:22</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">toys &amp; games</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">20</td><td style = \"text-align: left;\">Craneport</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">201.188.209.214</td><td style = \"text-align: left;\">Unit 7360 Box 5180\\nDPO AP 84328</td><td style = \"text-align: left;\">55423 Henry Haven\\nWesleychester, WI 59138</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">285</td><td style = \"text-align: right;\">22</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">0dae14e6-aca5-48b4-8853-51188d3e9d9c</td><td style = \"text-align: left;\">a30a5030-1c23-4750-a6b8-dd8fbf79c79b</td><td style = \"text-align: right;\">56.31</td><td style = \"text-align: left;\">2024-02-16 21:08:15</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">home &amp; garden</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">35</td><td style = \"text-align: left;\">West Michael</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">81.95.103.200</td><td style = \"text-align: left;\">3684 Morris Inlet Suite 155\\nChandlerville, NJ 01086</td><td style = \"text-align: left;\">3684 Morris Inlet Suite 155\\nChandlerville, NJ 01086</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">259</td><td style = \"text-align: right;\">21</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">fb09ac9b-8c76-4caf-9973-c79563a186c2</td><td style = \"text-align: left;\">2d86cadf-184e-46c3-9142-546fb584b3f8</td><td style = \"text-align: right;\">275.87</td><td style = \"text-align: left;\">2024-02-15 11:45:52</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">home &amp; garden</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">45</td><td style = \"text-align: left;\">Melindafurt</td><td style = \"text-align: left;\">mobile</td><td style = \"text-align: left;\">105.173.82.111</td><td style = \"text-align: left;\">4197 Lewis Way\\nMariachester, NC 04968</td><td style = \"text-align: left;\">075 Monroe Court\\nDavismouth, WA 67522</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">307</td><td style = \"text-align: right;\">11</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496575</td><td style = \"text-align: left;\">3b7bc8d4-3a56-42a0-aa81-cbea48815c90</td><td style = \"text-align: left;\">956fa6c1-5737-487c-9820-bf0e15b1926c</td><td style = \"text-align: right;\">102.15</td><td style = \"text-align: left;\">2024-01-04 13:20:39</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">health &amp; beauty</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">27</td><td style = \"text-align: left;\">New Shari</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">39.128.46.102</td><td style = \"text-align: left;\">1281 Robertson Villages\\nPort Janetchester, FL 21199</td><td style = \"text-align: left;\">567 Moore Islands\\nSouth Maryfort, TN 94628</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">76</td><td style = \"text-align: right;\">13</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496576</td><td style = \"text-align: left;\">ab782bd8-6596-4e10-9063-43c28af42717</td><td style = \"text-align: left;\">84ca799c-d467-4c3a-ab20-9d4c737fc81b</td><td style = \"text-align: right;\">69.37</td><td style = \"text-align: left;\">2024-01-11 05:35:46</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">toys &amp; games</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">36</td><td style = \"text-align: left;\">Parsonsstad</td><td style = \"text-align: left;\">mobile</td><td style = \"text-align: left;\">179.36.21.214</td><td style = \"text-align: left;\">78046 Bonilla Lodge Suite 065\\nPiercehaven, FM 05624</td><td style = \"text-align: left;\">78046 Bonilla Lodge Suite 065\\nPiercehaven, FM 05624</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">18</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496577</td><td style = \"text-align: left;\">e38659f8-37d5-415d-bbfb-443b715ca9f1</td><td style = \"text-align: left;\">286669b8-d86e-48b1-ab2c-277cd6d953bc</td><td style = \"text-align: right;\">146.67</td><td style = \"text-align: left;\">2024-03-11 19:02:25</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">electronics</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">42</td><td style = \"text-align: left;\">Aaronfurt</td><td style = \"text-align: left;\">mobile</td><td style = \"text-align: left;\">110.4.144.51</td><td style = \"text-align: left;\">478 James Falls Suite 662\\nLake Mark, ME 95615</td><td style = \"text-align: left;\">478 James Falls Suite 662\\nLake Mark, ME 95615</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">121</td><td style = \"text-align: right;\">19</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496578</td><td style = \"text-align: left;\">5e195aab-b17f-423a-ad7a-95f1a279603a</td><td style = \"text-align: left;\">90d18669-1500-4df5-a79d-b501393943cc</td><td style = \"text-align: right;\">215.02</td><td style = \"text-align: left;\">2024-01-10 09:37:25</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">home &amp; garden</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">20</td><td style = \"text-align: left;\">East Jameschester</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">54.29.161.167</td><td style = \"text-align: left;\">6289 Derek Burg Suite 204\\nMillerland, ME 14836</td><td style = \"text-align: left;\">6289 Derek Burg Suite 204\\nMillerland, ME 14836</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">271</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496579</td><td style = \"text-align: left;\">bddffc14-79b8-4433-945d-cc7aa14d5296</td><td style = \"text-align: left;\">d1fffbca-a84f-4d88-839f-9f7a50b67a7a</td><td style = \"text-align: right;\">65.87</td><td style = \"text-align: left;\">2024-03-25 10:47:41</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">health &amp; beauty</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">27</td><td style = \"text-align: left;\">South Deborah</td><td style = \"text-align: left;\">mobile</td><td style = \"text-align: left;\">47.87.10.169</td><td style = \"text-align: left;\">791 Johnson Burg\\nNew Richardfort, TX 28157</td><td style = \"text-align: left;\">757 Destiny Dam\\nLake Brian, OR 64783</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">154</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496580</td><td style = \"text-align: left;\">1449c5cf-fd30-47b3-bc42-55a26784bb33</td><td style = \"text-align: left;\">7b024b21-aa09-4e6d-b94f-8c7f704b4be6</td><td style = \"text-align: right;\">75.22</td><td style = \"text-align: left;\">2024-03-08 15:38:14</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">electronics</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">39</td><td style = \"text-align: left;\">Lake Michaelmouth</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">86.249.211.28</td><td style = \"text-align: left;\">630 Alvarez Fork Suite 639\\nLaurentown, VT 15862</td><td style = \"text-align: left;\">630 Alvarez Fork Suite 639\\nLaurentown, VT 15862</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">280</td><td style = \"text-align: right;\">15</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496581</td><td style = \"text-align: left;\">1bbd6772-57b0-4fa2-81f2-e1c7096eecc1</td><td style = \"text-align: left;\">04f68fcd-4642-4ef0-9549-e31505226550</td><td style = \"text-align: right;\">24.74</td><td style = \"text-align: left;\">2024-02-08 00:55:49</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">health &amp; beauty</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">25</td><td style = \"text-align: left;\">Blackburnview</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">161.23.44.204</td><td style = \"text-align: left;\">547 Lisa Wells\\nEast Victoriafurt, MP 49935</td><td style = \"text-align: left;\">22844 Gordon Burgs Suite 875\\nZacharyland, NE 31134</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">116</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496582</td><td style = \"text-align: left;\">d8b7171f-bdd9-479c-b98b-396c621aebfe</td><td style = \"text-align: left;\">98a3d94c-dc9a-4525-b273-e6ffe54cc5a4</td><td style = \"text-align: right;\">53.73</td><td style = \"text-align: left;\">2024-01-26 16:25:05</td><td style = \"text-align: left;\">PayPal</td><td style = \"text-align: left;\">toys &amp; games</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">32</td><td style = \"text-align: left;\">Rebeccafurt</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">173.97.197.128</td><td style = \"text-align: left;\">USNV Clayton\\nFPO AE 82639</td><td style = \"text-align: left;\">USNV Clayton\\nFPO AE 82639</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">363</td><td style = \"text-align: right;\">16</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496583</td><td style = \"text-align: left;\">0fd12cf3-c641-4499-8de1-15dc4555cb0c</td><td style = \"text-align: left;\">b3429f52-8b27-46b5-914c-4accd989edb4</td><td style = \"text-align: right;\">47.42</td><td style = \"text-align: left;\">2024-02-25 17:03:26</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">clothing</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">41</td><td style = \"text-align: left;\">Nataliefort</td><td style = \"text-align: left;\">desktop</td><td style = \"text-align: left;\">133.222.22.48</td><td style = \"text-align: left;\">9288 Patricia Cape Apt. 527\\nMelissaton, IL 38543</td><td style = \"text-align: left;\">9288 Patricia Cape Apt. 527\\nMelissaton, IL 38543</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">296</td><td style = \"text-align: right;\">17</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496584</td><td style = \"text-align: left;\">649680d3-a684-44cb-95bf-9b454c3aa86d</td><td style = \"text-align: left;\">066e25c9-4420-4224-bc3f-1a462708090e</td><td style = \"text-align: right;\">1045.23</td><td style = \"text-align: left;\">2024-03-28 23:46:47</td><td style = \"text-align: left;\">bank transfer</td><td style = \"text-align: left;\">health &amp; beauty</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">9</td><td style = \"text-align: left;\">East Shannonville</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">206.133.237.168</td><td style = \"text-align: left;\">3015 Elizabeth Summit Suite 819\\nEast Joelfort, IN 04473</td><td style = \"text-align: left;\">3015 Elizabeth Summit Suite 819\\nEast Joelfort, IN 04473</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">329</td><td style = \"text-align: right;\">23</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496585</td><td style = \"text-align: left;\">c10dbb08-28fc-4ec1-9850-d4e98d2b9640</td><td style = \"text-align: left;\">cde96e9c-f562-4b8c-8fa8-f356f474232b</td><td style = \"text-align: right;\">34.25</td><td style = \"text-align: left;\">2024-02-09 11:29:18</td><td style = \"text-align: left;\">debit card</td><td style = \"text-align: left;\">home &amp; garden</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">39</td><td style = \"text-align: left;\">Lake Nicole</td><td style = \"text-align: left;\">mobile</td><td style = \"text-align: left;\">16.204.137.130</td><td style = \"text-align: left;\">531 Brittany Pike\\nNew Stacy, OR 87952</td><td style = \"text-align: left;\">531 Brittany Pike\\nNew Stacy, OR 87952</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">347</td><td style = \"text-align: right;\">11</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1496586</td><td style = \"text-align: left;\">23e3c107-f2fc-48c2-abbc-7b809bf6f102</td><td style = \"text-align: left;\">d8d7a64e-8419-4421-910a-a7cf709a900b</td><td style = \"text-align: right;\">85.03</td><td style = \"text-align: left;\">2024-01-23 02:46:52</td><td style = \"text-align: left;\">credit card</td><td style = \"text-align: left;\">clothing</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">38</td><td style = \"text-align: left;\">Holtport</td><td style = \"text-align: left;\">tablet</td><td style = \"text-align: left;\">116.188.254.162</td><td style = \"text-align: left;\">289 Adams Wells\\nWest Joeltown, LA 69190</td><td style = \"text-align: left;\">289 Adams Wells\\nWest Joeltown, LA 69190</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">203</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Transaction ID & Customer ID & Transaction Amount & \\\\\n",
       "\t\\hline\n",
       "\t& String & String & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 15d2e414-8735-46fc-9e02-80b472b2580f & d1b87f62-51b2-493b-ad6a-77e0fe13e785 & 58.09 & $\\dots$ \\\\\n",
       "\t2 & 0bfee1a0-6d5e-40da-a446-d04e73b1b177 & 37de64d5-e901-4a56-9ea0-af0c24c069cf & 389.96 & $\\dots$ \\\\\n",
       "\t3 & e588eef4-b754-468e-9d90-d0e0abfc1af0 & 1bac88d6-4b22-409a-a06b-425119c57225 & 134.19 & $\\dots$ \\\\\n",
       "\t4 & 4de46e52-60c3-49d9-be39-636681009789 & 2357c76e-9253-4ceb-b44e-ef4b71cb7d4d & 226.17 & $\\dots$ \\\\\n",
       "\t5 & 074a76de-fe2d-443e-a00c-f044cdb68e21 & 45071bc5-9588-43ea-8093-023caec8ea1c & 121.53 & $\\dots$ \\\\\n",
       "\t6 & 4e707452-7c8a-4cbd-b0c1-2aeaa35c5e88 & 29616b04-2d5c-4729-9c9d-8d71a6ad9dc1 & 166.41 & $\\dots$ \\\\\n",
       "\t7 & 7ed952fe-8ae1-4f11-8cc5-6607060240d8 & fe21ae29-ba4c-424f-9d55-0095539c09fa & 92.88 & $\\dots$ \\\\\n",
       "\t8 & 0b2fb5aa-7171-472f-8269-371094608a07 & 024257c3-5671-4de8-a33c-98fc5cbe6f92 & 318.14 & $\\dots$ \\\\\n",
       "\t9 & 1f52366c-7f40-4397-885f-3856b6e6531c & f17640ca-49da-45d1-8461-c2a1cf9c1b61 & 47.92 & $\\dots$ \\\\\n",
       "\t10 & 3f10dfde-9c4c-4085-9872-4f6b39502ffb & aab93e75-582f-4455-80b4-1fb35733a47c & 121.78 & $\\dots$ \\\\\n",
       "\t11 & 75f19b14-516c-4f1c-a99e-f1c456453b55 & 6a2e1397-e24a-4145-a821-fd835f732369 & 633.39 & $\\dots$ \\\\\n",
       "\t12 & 0dae14e6-aca5-48b4-8853-51188d3e9d9c & a30a5030-1c23-4750-a6b8-dd8fbf79c79b & 56.31 & $\\dots$ \\\\\n",
       "\t13 & fb09ac9b-8c76-4caf-9973-c79563a186c2 & 2d86cadf-184e-46c3-9142-546fb584b3f8 & 275.87 & $\\dots$ \\\\\n",
       "\t14 & 3a25fa55-ec25-4b47-bb3a-21fd7b2a4833 & 7be37ed2-b2d6-48b2-850c-2ed88ee86695 & 178.94 & $\\dots$ \\\\\n",
       "\t15 & c696ffee-f01d-4447-b0d4-45fcbca65d13 & bffedad1-43c1-4ef5-945b-ead50f6e501c & 374.04 & $\\dots$ \\\\\n",
       "\t16 & e3ac696e-978c-43c9-a659-6e1a93289b28 & 4c80f103-ce9c-4e40-b1b2-b5a6c9d1f5ef & 169.04 & $\\dots$ \\\\\n",
       "\t17 & e3c1a8ee-6455-4a39-a4b0-6f3be08c56a8 & b4e261d2-2e81-4e57-b8a6-fa204ecd8a8b & 254.48 & $\\dots$ \\\\\n",
       "\t18 & 08ded43b-24bf-4001-925e-fd7ce210baa8 & ed5b5482-fa35-48b5-b029-ff5f53c951bf & 266.06 & $\\dots$ \\\\\n",
       "\t19 & 0c0345fa-406b-4064-8e57-750ea3b0ea41 & e852cd05-3ee9-45a3-9e2a-553d6d01e025 & 263.28 & $\\dots$ \\\\\n",
       "\t20 & 8501d4c2-10e4-4aea-88bc-10b2a168cb28 & 339683a0-3028-449e-a635-25f0e171ae16 & 475.76 & $\\dots$ \\\\\n",
       "\t21 & 4b798a5c-8400-439f-8e2c-491902645751 & 26695fcc-6a94-40e3-a9cf-698e355bdbdf & 89.38 & $\\dots$ \\\\\n",
       "\t22 & 0293493e-7c4f-4b2e-bcb0-838f380e9321 & aa5bb186-5693-40ee-b0ec-84d30b680849 & 264.24 & $\\dots$ \\\\\n",
       "\t23 & 402da312-6957-47a7-b513-40eb4a1c614e & 1967927c-2316-420b-ab36-8f34b5632463 & 123.35 & $\\dots$ \\\\\n",
       "\t24 & 77f985cd-261c-40c9-a2f9-b107491cddc9 & a2101086-0387-4968-8586-135049f91e61 & 199.02 & $\\dots$ \\\\\n",
       "\t25 & bf43217a-c0d3-4e77-bb23-7eb77c44d1b9 & 0d4ba3b2-a21e-411d-b136-fca763eb1c17 & 675.74 & $\\dots$ \\\\\n",
       "\t26 & b4c3e417-6627-449d-9923-8e554060a164 & de73f019-973d-4cfb-ba04-1529b33a496c & 352.67 & $\\dots$ \\\\\n",
       "\t27 & 0b5286d8-853b-4276-b3b5-7b20c309c2e3 & daebc1ea-4a8a-4029-99e3-cf5c9575c206 & 53.73 & $\\dots$ \\\\\n",
       "\t28 & 39b77e1a-f7aa-4281-b04c-8c401dc1b2b3 & 0e6c7829-1e94-44ee-b841-9b95fd7531f6 & 21.24 & $\\dots$ \\\\\n",
       "\t29 & 66eecfd6-fa41-4c3b-bab3-07c76ad7d798 & 67c1fbc9-5830-4095-9ea8-174cbf88ab27 & 118.83 & $\\dots$ \\\\\n",
       "\t30 & 1b46cabd-fa15-47b7-91aa-f61e7c2724c1 & a748a557-cf3d-4811-a1d1-811c7a7539e5 & 267.12 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1496586Ã—21 DataFrame\u001b[0m\n",
       "\u001b[1m     Row \u001b[0mâ”‚\u001b[1m Transaction ID                    \u001b[0m\u001b[1m Customer ID                     \u001b[0m â‹¯\n",
       "         â”‚\u001b[90m String                            \u001b[0m\u001b[90m String                          \u001b[0m â‹¯\n",
       "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "       1 â”‚ 15d2e414-8735-46fc-9e02-80b472b2â€¦  d1b87f62-51b2-493b-ad6a-77e0fe13 â‹¯\n",
       "       2 â”‚ 0bfee1a0-6d5e-40da-a446-d04e73b1â€¦  37de64d5-e901-4a56-9ea0-af0c24c0\n",
       "       3 â”‚ e588eef4-b754-468e-9d90-d0e0abfcâ€¦  1bac88d6-4b22-409a-a06b-425119c5\n",
       "       4 â”‚ 4de46e52-60c3-49d9-be39-63668100â€¦  2357c76e-9253-4ceb-b44e-ef4b71cb\n",
       "       5 â”‚ 074a76de-fe2d-443e-a00c-f044cdb6â€¦  45071bc5-9588-43ea-8093-023caec8 â‹¯\n",
       "       6 â”‚ 4e707452-7c8a-4cbd-b0c1-2aeaa35câ€¦  29616b04-2d5c-4729-9c9d-8d71a6ad\n",
       "       7 â”‚ 7ed952fe-8ae1-4f11-8cc5-66070602â€¦  fe21ae29-ba4c-424f-9d55-0095539c\n",
       "       8 â”‚ 0b2fb5aa-7171-472f-8269-37109460â€¦  024257c3-5671-4de8-a33c-98fc5cbe\n",
       "       9 â”‚ 1f52366c-7f40-4397-885f-3856b6e6â€¦  f17640ca-49da-45d1-8461-c2a1cf9c â‹¯\n",
       "      10 â”‚ 3f10dfde-9c4c-4085-9872-4f6b3950â€¦  aab93e75-582f-4455-80b4-1fb35733\n",
       "      11 â”‚ 75f19b14-516c-4f1c-a99e-f1c45645â€¦  6a2e1397-e24a-4145-a821-fd835f73\n",
       "    â‹®    â”‚                 â‹®                                  â‹®                â‹±\n",
       " 1496577 â”‚ e38659f8-37d5-415d-bbfb-443b715câ€¦  286669b8-d86e-48b1-ab2c-277cd6d9\n",
       " 1496578 â”‚ 5e195aab-b17f-423a-ad7a-95f1a279â€¦  90d18669-1500-4df5-a79d-b5013939 â‹¯\n",
       " 1496579 â”‚ bddffc14-79b8-4433-945d-cc7aa14dâ€¦  d1fffbca-a84f-4d88-839f-9f7a50b6\n",
       " 1496580 â”‚ 1449c5cf-fd30-47b3-bc42-55a26784â€¦  7b024b21-aa09-4e6d-b94f-8c7f704b\n",
       " 1496581 â”‚ 1bbd6772-57b0-4fa2-81f2-e1c7096eâ€¦  04f68fcd-4642-4ef0-9549-e3150522\n",
       " 1496582 â”‚ d8b7171f-bdd9-479c-b98b-396c621aâ€¦  98a3d94c-dc9a-4525-b273-e6ffe54c â‹¯\n",
       " 1496583 â”‚ 0fd12cf3-c641-4499-8de1-15dc4555â€¦  b3429f52-8b27-46b5-914c-4accd989\n",
       " 1496584 â”‚ 649680d3-a684-44cb-95bf-9b454c3aâ€¦  066e25c9-4420-4224-bc3f-1a462708\n",
       " 1496585 â”‚ c10dbb08-28fc-4ec1-9850-d4e98d2bâ€¦  cde96e9c-f562-4b8c-8fa8-f356f474\n",
       " 1496586 â”‚ 23e3c107-f2fc-48c2-abbc-7b809bf6â€¦  d8d7a64e-8419-4421-910a-a7cf709a â‹¯\n",
       "\u001b[36m                                             20 columns and 1496565 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#              DATA LOADING & 3-CLASS TARGET CREATION\n",
    "# ============================================================================\n",
    "\n",
    "const DATA_PATH = \"datasets/Fraudulent_E-Commerce_Transaction_Data_merge.csv\"\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ“‚ LOADING DATA\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "df = CSV.read(DATA_PATH, DataFrame)\n",
    "target_col = \"Is Fraudulent\"\n",
    "\n",
    "println(\"Original dataset size: $(size(df))\")\n",
    "println(\"Original fraud distribution:\")\n",
    "println(\"  Non-fraud: $(sum(df[!, target_col] .== 0))\")\n",
    "println(\"  Fraud:     $(sum(df[!, target_col] .== 1))\")\n",
    "\n",
    "# Create 3-class target\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸŽ¯ CREATING 3-CLASS TARGET\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "df_with_classes = create_risk_classes(df, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199d34c-9167-4dfd-92e2-d4fa2175fb37",
   "metadata": {},
   "source": [
    "## 2. Class Balancing & Train/Test Split\n",
    "\n",
    "**Challenge:** Highly imbalanced dataset (90% Legitimate, 8.6% Suspicious, 1.4% Fraudulent)\n",
    "\n",
    "**Solution:** Undersample majority classes to match minority class (20,654 samples per class)\n",
    "\n",
    "**Train/Test Split:**\n",
    "- **80% Training** (49,569 samples) - used for cross-validation and model selection\n",
    "- **20% Test** (12,393 samples) - held out for final evaluation\n",
    "\n",
    "**Critical:** Test set is NEVER used during training or model selection to prevent data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75628c65-341c-40c6-8e51-ddcc02957a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… TRAIN/TEST SPLIT (80% Train / 20% Test)\n",
      "======================================================================\n",
      "\n",
      "ðŸ”„ Balancing dataset...\n",
      "  Samples per class: 1000\n",
      "  Balanced dataset size: (3000, 21)\n",
      "\n",
      "ðŸ“Š Split Summary:\n",
      "  Total samples:     3000\n",
      "  Training set:      2400 (80%)\n",
      "  Test set:          600 (20%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#          CLASS BALANCING & TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"âœ… TRAIN/TEST SPLIT (80% Train / 20% Test)\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "# Balance classes\n",
    "class_0 = df_with_classes[df_with_classes.Risk_Class .== 0, :]\n",
    "class_1 = df_with_classes[df_with_classes.Risk_Class .== 1, :]\n",
    "class_2 = df_with_classes[df_with_classes.Risk_Class .== 2, :]\n",
    "\n",
    "n_min = minimum([size(class_0, 1), size(class_1, 1), size(class_2, 1)])\n",
    "n_target = min(n_min, 1000)\n",
    "\n",
    "println(\"\\nðŸ”„ Balancing dataset...\")\n",
    "println(\"  Samples per class: $n_target\")\n",
    "\n",
    "class_0_sample = class_0[shuffle(1:size(class_0, 1))[1:n_target], :]\n",
    "class_1_sample = class_1[shuffle(1:size(class_1, 1))[1:n_target], :]\n",
    "class_2_sample = class_2[shuffle(1:size(class_2, 1))[1:n_target], :]\n",
    "\n",
    "df_balanced = vcat(class_0_sample, class_1_sample, class_2_sample)\n",
    "df_balanced = df_balanced[shuffle(1:size(df_balanced, 1)), :]\n",
    "\n",
    "println(\"  Balanced dataset size: $(size(df_balanced))\")\n",
    "\n",
    "# Split Train/Test BEFORE preprocessing (critical!)\n",
    "n_total = size(df_balanced, 1)\n",
    "n_train = floor(Int, n_total * 0.80)\n",
    "n_test = n_total - n_train\n",
    "\n",
    "all_indices = shuffle(1:n_total)\n",
    "train_indices = all_indices[1:n_train]\n",
    "test_indices = all_indices[n_train+1:end]\n",
    "\n",
    "df_train = df_balanced[train_indices, :]\n",
    "df_test = df_balanced[test_indices, :]\n",
    "\n",
    "println(\"\\nðŸ“Š Split Summary:\")\n",
    "println(\"  Total samples:     $n_total\")\n",
    "println(\"  Training set:      $n_train (80%)\")\n",
    "println(\"  Test set:          $n_test (20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c850a-1ee0-47f8-8ba8-5ffbb6db6706",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Feature Engineering\n",
    "\n",
    "**Steps:**\n",
    "1. **Time Features:** Extract hour, create night flag (hour < 6)\n",
    "2. **Feature Engineering:**\n",
    "   - `Amount_per_AccountAge`: Transaction amount relative to account maturity\n",
    "   - `High_Value_Flag`: Transactions above 95th percentile\n",
    "   - `New_Account_Flag`: Accounts younger than 30 days\n",
    "3. **Missing Value Imputation:** Median imputation\n",
    "4. **Feature Selection:** Drop IDs, addresses, categorical features â†’ **8 numerical features**\n",
    "5. **Normalization:** Min-Max [0,1] using training set parameters only\n",
    "\n",
    "**Final Features (8):**\n",
    "- Transaction Amount\n",
    "- Account Age Days  \n",
    "- Transaction_Hour\n",
    "- Is_Night\n",
    "- Amount_per_AccountAge\n",
    "- High_Value_Flag\n",
    "- New_Account_Flag\n",
    "- (1 more from preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "491a8af7-0d2a-4e4a-a9f3-73416da93e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Preprocessing train and test sets...\n",
      "  Stats used for preprocessing (Calculated on Train):\n",
      "  Median Amount: 286.305\n",
      "  High Value Threshold (p95): 1753.9800000000005\n",
      "\n",
      "ðŸ“Š Preprocessed Data:\n",
      "  Features: 8\n",
      "  Train samples: 2400\n",
      "  Test samples: 600\n",
      "\n",
      "  Feature names: [\"Transaction Amount\", \"Account Age Days\", \"Transaction Hour\", \"Transaction_Hour\", \"Is_Night\", \"Amount_per_AccountAge\", \"High_Value_Flag\", \"New_Account_Flag\"]\n",
      "\n",
      "âœ… Cross-validation indices created (3 folds, stratified)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#                    PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\nðŸ”§ Preprocessing train and test sets...\")\n",
    "\n",
    "# 1. Fit & Transform sul Train Set\n",
    "# Otteniamo sia il dataframe processato CHE le statistiche calcolate (train_stats)\n",
    "df_train_processed, train_stats = preprocess_multiclass(df_train, target_col)\n",
    "\n",
    "# 2. Transform sul Test Set\n",
    "# Passiamo 'train_stats' per forzare l'uso delle statistiche del train (mediane, percentili)\n",
    "# in modo che il test set non \"contamini\" il processo.\n",
    "df_test_processed = preprocess_multiclass(df_test, target_col; stats=train_stats)\n",
    "\n",
    "println(\"  Stats used for preprocessing (Calculated on Train):\")\n",
    "println(\"  Median Amount: $(train_stats[\"Transaction Amount_median\"])\")\n",
    "println(\"  High Value Threshold (p95): $(train_stats[\"amount_p95\"])\")\n",
    "\n",
    "input_cols = setdiff(names(df_train_processed), [\"Risk_Class\"])\n",
    "train_inputs = Matrix{Float32}(df_train_processed[:, input_cols])\n",
    "train_targets = Int.(df_train_processed.Risk_Class)\n",
    "\n",
    "test_inputs = Matrix{Float32}(df_test_processed[:, input_cols])\n",
    "test_targets = Int.(df_test_processed.Risk_Class)\n",
    "\n",
    "println(\"\\nðŸ“Š Preprocessed Data:\")\n",
    "println(\"  Features: $(length(input_cols))\")\n",
    "println(\"  Train samples: $(size(train_inputs, 1))\")\n",
    "println(\"  Test samples: $(size(test_inputs, 1))\")\n",
    "println(\"\\n  Feature names: $input_cols\")\n",
    "\n",
    "# Create cross-validation indices (3-fold stratified)\n",
    "k_folds = 3\n",
    "cv_indices = crossvalidation(train_targets, k_folds)\n",
    "println(\"\\nâœ… Cross-validation indices created ($k_folds folds, stratified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e7d2f-dda1-4c71-8050-1ac38fd37097",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: Artificial Neural Networks (ANNs)\n",
    "\n",
    "**Configuration:**\n",
    "- **Topologies tested:** 8 architectures (1-4 hidden layers)\n",
    "- **Activation:** ReLU (hidden layers), Softmax (output)\n",
    "- **Optimizer:** Adam (learning rate: 0.003)\n",
    "- **Loss:** Cross-entropy\n",
    "- **Regularization:** Early stopping (patience: 25 epochs)\n",
    "- **Validation:** 10% of training set\n",
    "- **Executions:** 1 per topology (for speed; can increase for stability)\n",
    "\n",
    "**Architectures:**\n",
    "1. `[256]` - Large\n",
    "2. `[128]` - Medium\n",
    "3. `[64]` - Small\n",
    "4. `[32]` - Tiny\n",
    "5. `[256, 128]` - Large 2-layer\n",
    "6. `[128, 64]` - Medium 2-layer\n",
    "7. `[64, 32]` - Small 2-layer\n",
    "8. `[96, 48]` - Alternative 2-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23091362-9363-4891-a1d9-46ffb4d9a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”¬ EXPERIMENT 1: ARTIFICIAL NEURAL NETWORKS\n",
      "Testing 8 ANN Topologies\n",
      "======================================================================\n",
      "\n",
      "[1/8] Testing topology: [256]\n",
      "    F1: 73.34% Â± 1.32%\n",
      "\n",
      "[2/8] Testing topology: [128]\n",
      "    F1: 73.18% Â± 2.15%\n",
      "\n",
      "[3/8] Testing topology: [64]\n",
      "    F1: 71.76% Â± 1.18%\n",
      "\n",
      "[4/8] Testing topology: [32]\n",
      "    F1: 71.79% Â± 1.52%\n",
      "\n",
      "[5/8] Testing topology: [256, 128]\n",
      "    F1: 72.49% Â± 1.47%\n",
      "\n",
      "[6/8] Testing topology: [128, 64]\n",
      "    F1: 73.73% Â± 1.11%\n",
      "\n",
      "[7/8] Testing topology: [64, 32]\n",
      "    F1: 71.69% Â± 0.51%\n",
      "\n",
      "[8/8] Testing topology: [96, 48]\n",
      "    F1: 73.16% Â± 1.18%\n",
      "\n",
      "ðŸ† ANN Results Ranking (by F1 Score):\n",
      "----------------------------------------------------------------------\n",
      "ðŸ¥‡ 1. [128, 64] - F1: 73.73%\n",
      "ðŸ¥ˆ 2. [256] - F1: 73.34%\n",
      "ðŸ¥‰ 3. [128] - F1: 73.18%\n",
      "   4. [96, 48] - F1: 73.16%\n",
      "   5. [256, 128] - F1: 72.49%\n",
      "   6. [32] - F1: 71.79%\n",
      "   7. [64] - F1: 71.76%\n",
      "   8. [64, 32] - F1: 71.69%\n",
      "\n",
      "âœ¨ Best ANN: [128, 64] (CV F1: 73.73%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#        EXPERIMENT 1: ARTIFICIAL NEURAL NETWORKS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 1: ARTIFICIAL NEURAL NETWORKS\")\n",
    "println(\"Testing 8 ANN Topologies\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "topologies_to_test = [\n",
    "    [256],            # 1. 1 hidden layer - Large\n",
    "    [128],            # 2. 1 hidden layer - Medium\n",
    "    [64],             # 3. 1 hidden layer - Small\n",
    "    [32],             # 4. 1 hidden layer - Tiny\n",
    "    [256, 128],       # 5. 2 hidden layers - Large\n",
    "    [128, 64],        # 6. 2 hidden layers - Medium\n",
    "    [64, 32],         # 7. 2 hidden layers - Small\n",
    "    [96, 48]          # 8. 2 hidden layers - Alternative\n",
    "]\n",
    "\n",
    "ann_results = []\n",
    "\n",
    "for (i, topology) in enumerate(topologies_to_test)\n",
    "    println(\"\\n[$i/8] Testing topology: $topology\")\n",
    "    \n",
    "    hyperparams = Dict(\n",
    "        \"topology\" => topology,\n",
    "        \"learningRate\" => 0.003,\n",
    "        \"validationRatio\" => 0.1,\n",
    "        \"numExecutions\" => 1,\n",
    "        \"maxEpochs\" => 800,\n",
    "        \"maxEpochsVal\" => 25\n",
    "    )\n",
    "    \n",
    "    # Use modelCrossValidation from utils.jl\n",
    "    results = modelCrossValidation(\n",
    "        :ANN,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    \n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(ann_results, (topology, f1_stats[1], results))\n",
    "end\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_ann_results = sort(ann_results, by=x->x[2], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† ANN Results Ranking (by F1 Score):\")\n",
    "println(\"-\"^70)\n",
    "for (i, (topo, f1, _)) in enumerate(sorted_ann_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. $topo - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_topology_ann = sorted_ann_results[1][1]\n",
    "best_f1_ann = sorted_ann_results[1][2]\n",
    "println(\"\\nâœ¨ Best ANN: $best_topology_ann (CV F1: $(round(best_f1_ann*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d46b960-4937-483a-bcc4-4a02baf19215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training final ANN on full training set (Retrain + Full Metrics)...\n",
      "ðŸ“Š ANN Test Results: Acc=0.715, F1=0.7\n"
     ]
    }
   ],
   "source": [
    "# Train final ANN on full training set and evaluate on test set\n",
    "println(\"\\nðŸš€ Training final ANN on full training set (Retrain + Full Metrics)...\")\n",
    "\n",
    "# Setup Data for Final Training\n",
    "classes_ann = sort(unique(train_targets))\n",
    "train_targets_onehot = oneHotEncoding(train_targets, classes_ann)\n",
    "test_targets_onehot = oneHotEncoding(test_targets, classes_ann)\n",
    "\n",
    "normParams_ann = calculateMinMaxNormalizationParameters(train_inputs)\n",
    "train_inputs_norm = normalizeMinMax(train_inputs, normParams_ann)\n",
    "test_inputs_norm = normalizeMinMax(test_inputs, normParams_ann)\n",
    "\n",
    "# Validation split for Early Stopping\n",
    "N_train = size(train_inputs_norm, 1)\n",
    "(train_idx, val_idx) = holdOut(N_train, 0.1)\n",
    "\n",
    "final_ann, _ = _trainClassANN(best_topology_ann,\n",
    "    (train_inputs_norm[train_idx, :], train_targets_onehot[train_idx, :]),\n",
    "    validationDataset=(train_inputs_norm[val_idx, :], train_targets_onehot[val_idx, :]),\n",
    "    testDataset=(test_inputs_norm, test_targets_onehot),\n",
    "    maxEpochs=800, learningRate=0.003, maxEpochsVal=25)\n",
    "\n",
    "# Predict (Raw Probabilities) -> Needed for Ensemble later!\n",
    "test_outputs_ann = final_ann(test_inputs_norm')'\n",
    "\n",
    "# Calculate Full Metrics\n",
    "preds_ann_cls = classifyOutputs(test_outputs_ann) # Boolean matrix\n",
    "preds_ann_int = [findfirst(x->x, row) - 1 for row in eachrow(preds_ann_cls)] # 0,1,2 labels\n",
    "\n",
    "# Prepare args for metrics helper\n",
    "probs_ann_vec = (size(test_targets_onehot, 2) == 1) ? vec(test_outputs_ann) : test_outputs_ann\n",
    "metrics_ann = calculate_metrics_safe(probs_ann_vec, preds_ann_int, test_targets, test_targets_onehot, classes_ann)\n",
    "\n",
    "println(\"ðŸ“Š ANN Test Results: Acc=$(round(metrics_ann[\"Accuracy\"],digits=3)), F1=$(round(metrics_ann[\"F1\"],digits=3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50702451-b1b3-4ccd-b022-2432a2e0e76d",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2: Support Vector Machines (SVMs)\n",
    "\n",
    "**Configuration:**\n",
    "- **10 configurations tested**\n",
    "- **Kernels:** Linear, RBF, Polynomial\n",
    "- **Hyperparameter C:** 0.1, 1.0, 10.0\n",
    "- **Gamma (RBF):** auto (1/n_features), 0.1\n",
    "- **Degree (Polynomial):** 2, 3\n",
    "\n",
    "**Configurations:**\n",
    "1-3. Linear (C = 0.1, 1.0, 10.0)\n",
    "4-6. RBF with auto gamma (C = 0.1, 1.0, 10.0)\n",
    "7. RBF with Î³=0.1 (C = 1.0)\n",
    "8-10. Polynomial degree 2, 3 (C = 1.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31ff0a3-d803-4e0c-9e27-66b359bb067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”¬ EXPERIMENT 2: SUPPORT VECTOR MACHINES\n",
      "Testing 10 SVM Configurations\n",
      "======================================================================\n",
      "\n",
      "[1/10] Testing: Linear C=0.1\n",
      "    F1: 61.21% Â± 0.13%\n",
      "\n",
      "[2/10] Testing: Linear C=1.0\n",
      "    F1: 65.33% Â± 1.02%\n",
      "\n",
      "[3/10] Testing: Linear C=10.0\n",
      "    F1: 74.73% Â± 0.65%\n",
      "\n",
      "[4/10] Testing: RBF C=0.1 Î³=auto\n",
      "    F1: 52.16% Â± 3.65%\n",
      "\n",
      "[5/10] Testing: RBF C=1.0 Î³=auto\n",
      "    F1: 63.98% Â± 1.08%\n",
      "\n",
      "[6/10] Testing: RBF C=10.0 Î³=auto\n",
      "    F1: 73.68% Â± 2.01%\n",
      "\n",
      "[7/10] Testing: RBF C=1.0 Î³=0.1\n",
      "    F1: 63.66% Â± 1.03%\n",
      "\n",
      "[8/10] Testing: Poly C=1.0 deg=2\n",
      "    F1: 56.36% Â± 1.07%\n",
      "\n",
      "[9/10] Testing: Poly C=1.0 deg=3\n",
      "    F1: 48.78% Â± 1.06%\n",
      "\n",
      "[10/10] Testing: Poly C=10.0 deg=2\n",
      "    F1: 62.78% Â± 0.92%\n",
      "\n",
      "ðŸ† SVM Results Ranking:\n",
      "----------------------------------------------------------------------\n",
      "ðŸ¥‡ 1. Linear C=10.0 - F1: 74.73%\n",
      "ðŸ¥ˆ 2. RBF C=10.0 Î³=auto - F1: 73.68%\n",
      "ðŸ¥‰ 3. Linear C=1.0 - F1: 65.33%\n",
      "   4. RBF C=1.0 Î³=auto - F1: 63.98%\n",
      "   5. RBF C=1.0 Î³=0.1 - F1: 63.66%\n",
      "   6. Poly C=10.0 deg=2 - F1: 62.78%\n",
      "   7. Linear C=0.1 - F1: 61.21%\n",
      "   8. Poly C=1.0 deg=2 - F1: 56.36%\n",
      "   9. RBF C=0.1 Î³=auto - F1: 52.16%\n",
      "   10. Poly C=1.0 deg=3 - F1: 48.78%\n",
      "\n",
      "âœ¨ Best SVM: Linear C=10.0 (CV F1: 74.73%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#        EXPERIMENT 2: SUPPORT VECTOR MACHINES\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 2: SUPPORT VECTOR MACHINES\")\n",
    "println(\"Testing 10 SVM Configurations\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "svm_configs = [\n",
    "    (\"linear\", 0.1, 0.125, 3, \"Linear C=0.1\"),\n",
    "    (\"linear\", 1.0, 0.125, 3, \"Linear C=1.0\"),\n",
    "    (\"linear\", 10.0, 0.125, 3, \"Linear C=10.0\"),\n",
    "    (\"rbf\", 0.1, 0.125, 3, \"RBF C=0.1 Î³=auto\"),\n",
    "    (\"rbf\", 1.0, 0.125, 3, \"RBF C=1.0 Î³=auto\"),\n",
    "    (\"rbf\", 10.0, 0.125, 3, \"RBF C=10.0 Î³=auto\"),\n",
    "    (\"rbf\", 1.0, 0.1, 3, \"RBF C=1.0 Î³=0.1\"),\n",
    "    (\"poly\", 1.0, 0.125, 2, \"Poly C=1.0 deg=2\"),\n",
    "    (\"poly\", 1.0, 0.125, 3, \"Poly C=1.0 deg=3\"),\n",
    "    (\"poly\", 10.0, 0.125, 2, \"Poly C=10.0 deg=2\")\n",
    "]\n",
    "\n",
    "svm_results = []\n",
    "\n",
    "for (i, (kernel, C, gamma, degree, desc)) in enumerate(svm_configs)\n",
    "    println(\"\\n[$i/10] Testing: $desc\")\n",
    "    \n",
    "    hyperparams = Dict(\n",
    "        \"kernel\" => kernel,\n",
    "        \"C\" => C,\n",
    "        \"gamma\" => gamma,\n",
    "        \"degree\" => degree\n",
    "    )\n",
    "    \n",
    "    results = modelCrossValidation(\n",
    "        :SVC,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(svm_results, (desc, f1_stats[1], kernel, C, gamma, degree, results))\n",
    "end\n",
    "\n",
    "sorted_svm_results = sort(svm_results, by=x->x[2], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† SVM Results Ranking:\")\n",
    "println(\"-\"^70)\n",
    "for (i, (desc, f1, _, _, _, _, _)) in enumerate(sorted_svm_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. $desc - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_svm = sorted_svm_results[1]\n",
    "best_desc_svm, best_f1_svm, best_kernel_svm, best_C_svm, best_gamma_svm, best_degree_svm = best_svm[1:6]\n",
    "println(\"\\nâœ¨ Best SVM: $best_desc_svm (CV F1: $(round(best_f1_svm*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ceeb5f-483f-4bd8-8d31-7293fa617911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training final SVM on full training set...\n",
      "ðŸ“Š SVM Test Results: Acc=0.722, F1=0.711\n"
     ]
    }
   ],
   "source": [
    "# Train final SVM and evaluate on test set\n",
    "\n",
    "println(\"\\nðŸš€ Training final SVM on full training set...\")\n",
    "\n",
    "# Setup Data\n",
    "train_inputs_norm = normalizeMinMax(train_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "test_inputs_norm = normalizeMinMax(test_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "train_targets_str = string.(train_targets)\n",
    "test_targets_str = string.(test_targets)\n",
    "classes_str = sort(unique(train_targets_str))\n",
    "\n",
    "# Train\n",
    "(k, C, g, d) = (best_kernel_svm, best_C_svm, best_gamma_svm, best_degree_svm)\n",
    "k_func = k == \"linear\" ? LIBSVM.Kernel.Linear : (k == \"poly\" ? LIBSVM.Kernel.Polynomial : LIBSVM.Kernel.RadialBasis)\n",
    "model_svm = SVMClassifier(kernel=k_func, cost=C, gamma=g, degree=Int32(d))\n",
    "mach_svm = machine(model_svm, MLJ.table(train_inputs_norm), categorical(train_targets_str))\n",
    "MLJ.fit!(mach_svm, verbosity=0)\n",
    "\n",
    "# Predict -> Needed for Ensemble!\n",
    "svm_predictions = MLJ.predict(mach_svm, MLJ.table(test_inputs_norm))\n",
    "svm_predictions_str = string.(svm_predictions)\n",
    "\n",
    "# Calculate Metrics\n",
    "# SVM probabilities are tricky with LIBSVM wrapper, passing zeros for AUC (placeholder)\n",
    "metrics_svm = calculate_metrics_safe(zeros(length(svm_predictions_str)), svm_predictions_str, test_targets_str, test_targets_onehot, classes_str)\n",
    "\n",
    "println(\"ðŸ“Š SVM Test Results: Acc=$(round(metrics_svm[\"Accuracy\"],digits=3)), F1=$(round(metrics_svm[\"F1\"],digits=3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fb632-2fd0-41d4-91d4-9a075baeb3c9",
   "metadata": {},
   "source": [
    "# EXPERIMENT 3: Decision Trees\n",
    "\n",
    "**Configuration:**\n",
    "- **7 maximum depths tested:** 3, 5, 7, 10, 15, 20, unlimited\n",
    "- **Splitting criterion:** Gini impurity\n",
    "- **Min samples split:** 2\n",
    "- **Random seed:** 42 (for reproducibility)\n",
    "\n",
    "**Advantages:**\n",
    "- Interpretable (can visualize decision rules)\n",
    "- No feature scaling required\n",
    "- Fast training and prediction\n",
    "- Handles non-linear relationships naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde40a03-3944-41b6-bdfa-626ef00b505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”¬ EXPERIMENT 3: DECISION TREES\n",
      "Testing 7 Maximum Depths\n",
      "======================================================================\n",
      "\n",
      "[1/7] Testing: Depth=3\n",
      "    F1: 66.35% Â± 1.1%\n",
      "\n",
      "[2/7] Testing: Depth=5\n",
      "    F1: 72.85% Â± 1.33%\n",
      "\n",
      "[3/7] Testing: Depth=7\n",
      "    F1: 74.23% Â± 0.51%\n",
      "\n",
      "[4/7] Testing: Depth=10\n",
      "    F1: 72.13% Â± 2.26%\n",
      "\n",
      "[5/7] Testing: Depth=15\n",
      "    F1: 71.41% Â± 1.2%\n",
      "\n",
      "[6/7] Testing: Depth=20\n",
      "    F1: 71.4% Â± 1.97%\n",
      "\n",
      "[7/7] Testing: Depth=Unlimited\n",
      "    F1: 71.1% Â± 1.25%\n",
      "\n",
      "ðŸ† Decision Tree Results Ranking:\n",
      "----------------------------------------------------------------------\n",
      "ðŸ¥‡ 1. Depth=7 - F1: 74.23%\n",
      "ðŸ¥ˆ 2. Depth=5 - F1: 72.85%\n",
      "ðŸ¥‰ 3. Depth=10 - F1: 72.13%\n",
      "   4. Depth=15 - F1: 71.41%\n",
      "   5. Depth=20 - F1: 71.4%\n",
      "   6. Depth=Unlimited - F1: 71.1%\n",
      "   7. Depth=3 - F1: 66.35%\n",
      "\n",
      "âœ¨ Best Tree: Depth=7 (CV F1: 74.23%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#            EXPERIMENT 3: DECISION TREES\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 3: DECISION TREES\")\n",
    "println(\"Testing 7 Maximum Depths\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "tree_depths = [3, 5, 7, 10, 15, 20, -1]\n",
    "tree_results = []\n",
    "\n",
    "for (i, max_depth) in enumerate(tree_depths)\n",
    "    depth_str = max_depth == -1 ? \"Unlimited\" : string(max_depth)\n",
    "    println(\"\\n[$i/7] Testing: Depth=$depth_str\")\n",
    "    \n",
    "    hyperparams = Dict(\"max_depth\" => max_depth)\n",
    "    \n",
    "    results = modelCrossValidation(\n",
    "        :DecisionTreeClassifier,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(tree_results, (depth_str, max_depth, f1_stats[1], results))\n",
    "end\n",
    "\n",
    "sorted_tree_results = sort(tree_results, by=x->x[3], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† Decision Tree Results Ranking:\")\n",
    "println(\"-\"^70)\n",
    "for (i, (depth_str, _, f1, _)) in enumerate(sorted_tree_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. Depth=$depth_str - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_desc_tree, best_max_depth_tree, best_f1_tree = sorted_tree_results[1][1:3]\n",
    "println(\"\\nâœ¨ Best Tree: Depth=$best_desc_tree (CV F1: $(round(best_f1_tree*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a977cf18-f4e3-4169-b051-671fe2b62ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training final Decision Tree...\n",
      "ðŸ“Š DT Test Results: Acc=0.747, F1=0.737\n"
     ]
    }
   ],
   "source": [
    "# Train final Decision Tree\n",
    "\n",
    "println(\"\\nðŸš€ Training final Decision Tree...\")\n",
    "\n",
    "model_tree = DTClassifier(max_depth=best_max_depth_tree, rng=Random.MersenneTwister(42))\n",
    "mach_tree = machine(model_tree, MLJ.table(train_inputs_norm), categorical(train_targets_str))\n",
    "MLJ.fit!(mach_tree, verbosity=0)\n",
    "\n",
    "# Predict -> Needed for Ensemble!\n",
    "tree_predictions = MLJ.predict(mach_tree, MLJ.table(test_inputs_norm))\n",
    "tree_predictions_mode = mode.(tree_predictions)\n",
    "tree_predictions_str = string.(tree_predictions_mode)\n",
    "\n",
    "# Calculate Metrics (Attempt extracting prob for AUC)\n",
    "probs_dt = zeros(length(tree_predictions_str))\n",
    "try; probs_dt = pdf.(tree_predictions, classes_str[end]); catch; end\n",
    "\n",
    "metrics_tree = calculate_metrics_safe(probs_dt, tree_predictions_str, test_targets_str, test_targets_onehot, classes_str)\n",
    "\n",
    "println(\"ðŸ“Š DT Test Results: Acc=$(round(metrics_tree[\"Accuracy\"],digits=3)), F1=$(round(metrics_tree[\"F1\"],digits=3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83fd2b-e43e-43c7-bd54-1f8e3e86951b",
   "metadata": {},
   "source": [
    "# EXPERIMENT 4: k-Nearest Neighbors (kNN)\n",
    "\n",
    "**Configuration:**\n",
    "- **6 k values tested:** 1, 3, 5, 7, 10, 15\n",
    "- **Distance metric:** Euclidean\n",
    "- **Voting:** Majority voting among k neighbors\n",
    "\n",
    "**Notes:**\n",
    "- Feature normalization is CRITICAL for kNN (distance-based)\n",
    "- No explicit training phase (lazy learning)\n",
    "- k=1 is most sensitive to noise\n",
    "- Higher k values create smoother decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fbc4c1-ea77-47cf-bf63-7090778cfd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”¬ EXPERIMENT 4: k-NEAREST NEIGHBORS\n",
      "Testing 6 k Values\n",
      "======================================================================\n",
      "\n",
      "[1/6] Testing: k=1\n",
      "    F1: 70.68% Â± 1.2%\n",
      "\n",
      "[2/6] Testing: k=3\n",
      "    F1: 71.38% Â± 1.83%\n",
      "\n",
      "[3/6] Testing: k=5\n",
      "    F1: 71.98% Â± 0.91%\n",
      "\n",
      "[4/6] Testing: k=7\n",
      "    F1: 71.85% Â± 1.08%\n",
      "\n",
      "[5/6] Testing: k=10\n",
      "    F1: 70.63% Â± 1.31%\n",
      "\n",
      "[6/6] Testing: k=15\n",
      "    F1: 70.44% Â± 0.63%\n",
      "\n",
      "ðŸ† kNN Results Ranking:\n",
      "----------------------------------------------------------------------\n",
      "ðŸ¥‡ 1. k=5 - F1: 71.98%\n",
      "ðŸ¥ˆ 2. k=7 - F1: 71.85%\n",
      "ðŸ¥‰ 3. k=3 - F1: 71.38%\n",
      "   4. k=1 - F1: 70.68%\n",
      "   5. k=10 - F1: 70.63%\n",
      "   6. k=15 - F1: 70.44%\n",
      "\n",
      "âœ¨ Best kNN: k=5 (CV F1: 71.98%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#            EXPERIMENT 4: k-NEAREST NEIGHBORS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 4: k-NEAREST NEIGHBORS\")\n",
    "println(\"Testing 6 k Values\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "k_values = [1, 3, 5, 7, 10, 15]\n",
    "knn_results = []\n",
    "\n",
    "for (i, k) in enumerate(k_values)\n",
    "    println(\"\\n[$i/6] Testing: k=$k\")\n",
    "    \n",
    "    hyperparams = Dict(\"n_neighbors\" => k)\n",
    "    \n",
    "    results = modelCrossValidation(\n",
    "        :KNeighborsClassifier,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(knn_results, (k, f1_stats[1], results))\n",
    "end\n",
    "\n",
    "sorted_knn_results = sort(knn_results, by=x->x[2], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† kNN Results Ranking:\")\n",
    "println(\"-\"^70)\n",
    "for (i, (k, f1, _)) in enumerate(sorted_knn_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. k=$k - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_k_knn, best_f1_knn = sorted_knn_results[1][1:2]\n",
    "println(\"\\nâœ¨ Best kNN: k=$best_k_knn (CV F1: $(round(best_f1_knn*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb3e67c-a3f3-4ddf-8897-e456f64491c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Preparing final kNN...\n",
      "ðŸ“Š kNN Test Results: Acc=0.727, F1=0.72\n"
     ]
    }
   ],
   "source": [
    "# Train final kNN\n",
    "println(\"\\nðŸš€ Preparing final kNN...\")\n",
    "\n",
    "model_knn = kNNClassifier(K=best_k_knn)\n",
    "mach_knn = machine(model_knn, MLJ.table(train_inputs_norm), categorical(train_targets_str))\n",
    "MLJ.fit!(mach_knn, verbosity=0)\n",
    "\n",
    "# Predict -> Needed for Ensemble!\n",
    "knn_predictions = MLJ.predict(mach_knn, MLJ.table(test_inputs_norm))\n",
    "knn_predictions_mode = mode.(knn_predictions)\n",
    "knn_predictions_str = string.(knn_predictions_mode)\n",
    "\n",
    "# Calculate Metrics\n",
    "probs_knn = zeros(length(knn_predictions_str))\n",
    "try; probs_knn = pdf.(knn_predictions, classes_str[end]); catch; end\n",
    "\n",
    "metrics_knn = calculate_metrics_safe(probs_knn, knn_predictions_str, test_targets_str, test_targets_onehot, classes_str)\n",
    "\n",
    "println(\"ðŸ“Š kNN Test Results: Acc=$(round(metrics_knn[\"Accuracy\"],digits=3)), F1=$(round(metrics_knn[\"F1\"],digits=3))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735d62e-b4f7-4684-91c1-61f8b717a1bd",
   "metadata": {},
   "source": [
    "# EXPERIMENT 5: Ensemble Methods\n",
    "\n",
    "**Strategy:** Combine the top 3 individual models to improve robustness\n",
    "\n",
    "**Models Selected:**\n",
    "1. Best ANN\n",
    "2. Best Decision Tree\n",
    "3. Best kNN\n",
    "\n",
    "**Ensemble Techniques:**\n",
    "1. **Majority Voting:** Each model votes equally, winner takes all\n",
    "2. **Weighted Voting:** Models vote proportionally to their CV F1 scores\n",
    "\n",
    "**Expected Benefits:**\n",
    "- Reduced variance through model averaging\n",
    "- More robust predictions\n",
    "- Leverage complementary strengths of different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370a6fc5-c61b-4529-8ac9-44d7266a7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”¬ EXPERIMENT 5: ENSEMBLE METHODS\n",
      "Combining ANN + Decision Tree + kNN\n",
      "======================================================================\n",
      "\n",
      "[1/2] Majority Voting...\n",
      "âœ… Majority Voting Results:\n",
      "   F1: 72.86%\n",
      "   Acc: 74.0%\n",
      "\n",
      "[2/2] Weighted Voting...\n",
      "  Model weights:\n",
      "    ANN: 32.5%\n",
      "    DT:  34.2%\n",
      "    kNN: 33.4%\n",
      "âœ… Weighted Voting Results:\n",
      "   F1: 73.05%\n",
      "   Acc: 74.17%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#            EXPERIMENT 5: ENSEMBLE METHODS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 5: ENSEMBLE METHODS\")\n",
    "println(\"Combining ANN + Decision Tree + kNN\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "# Helper functions for ensemble\n",
    "function majorityVoting(predictions::Vector{Vector{String}})\n",
    "    n_samples = length(predictions[1])\n",
    "    ensemble_predictions = Vector{String}(undef, n_samples)\n",
    "    for i in 1:n_samples\n",
    "        votes = [pred[i] for pred in predictions]\n",
    "        ensemble_predictions[i] = mode(votes)\n",
    "    end\n",
    "    return ensemble_predictions\n",
    "end\n",
    "\n",
    "function weightedVoting(predictions::Vector{Vector{String}}, weights::Vector{Float64})\n",
    "    n_samples = length(predictions[1])\n",
    "    n_models = length(predictions)\n",
    "    # Ensure classes are gathered from all predictions\n",
    "    classes_unique = sort(unique(vcat(predictions...)))\n",
    "    \n",
    "    ensemble_predictions = Vector{String}(undef, n_samples)\n",
    "    for i in 1:n_samples\n",
    "        class_scores = Dict(c => 0.0 for c in classes_unique)\n",
    "        for j in 1:n_models\n",
    "            class_pred = predictions[j][i]\n",
    "            if haskey(class_scores, class_pred)\n",
    "                class_scores[class_pred] += weights[j]\n",
    "            end\n",
    "        end\n",
    "        ensemble_predictions[i] = argmax(class_scores)\n",
    "    end\n",
    "    return ensemble_predictions\n",
    "end\n",
    "\n",
    "# 1. Prepare Predictions (Ensure all are Strings)\n",
    "# ANN outputs (calculated in Exp 1 update) -> Convert to String labels\n",
    "# We use preds_ann_int calculated previously (0, 1, 2 integers)\n",
    "ann_test_pred_str = string.(preds_ann_int) \n",
    "\n",
    "# SVM outputs (already string from Exp 2)\n",
    "svm_test_pred_str = svm_predictions_str\n",
    "\n",
    "# DT outputs (already string from Exp 3)\n",
    "tree_test_pred_str = tree_predictions_str\n",
    "\n",
    "# kNN outputs (already string from Exp 4)\n",
    "knn_test_pred_str = knn_predictions_str\n",
    "\n",
    "# 2. Select Models for Ensemble (Top 3 usually, or all 4)\n",
    "# Let's use ANN, DT, and kNN as per original plan (or add SVM if you wish)\n",
    "all_predictions = [ann_test_pred_str, tree_test_pred_str, knn_test_pred_str]\n",
    "model_names = [\"ANN\", \"DT\", \"kNN\"]\n",
    "\n",
    "# 3. Define Classes as Strings (CRITICAL FIX)\n",
    "classes_str = sort(unique(vcat(all_predictions...)))\n",
    "test_targets_str = string.(test_targets) # Ensure targets are strings too\n",
    "\n",
    "# --- Method 1: Majority Voting ---\n",
    "println(\"\\n[1/2] Majority Voting...\")\n",
    "majority_predictions = majorityVoting(all_predictions)\n",
    "cm_results_majority = confusionMatrix(majority_predictions, test_targets_str, classes_str; weighted=true)\n",
    "\n",
    "println(\"âœ… Majority Voting Results:\")\n",
    "println(\"   F1: $(round(cm_results_majority.aggregated.f1*100, digits=2))%\")\n",
    "println(\"   Acc: $(round(cm_results_majority.accuracy*100, digits=2))%\")\n",
    "\n",
    "# --- Method 2: Weighted Voting ---\n",
    "println(\"\\n[2/2] Weighted Voting...\")\n",
    "# Retrieve F1 scores from the metrics calculated in previous steps\n",
    "w_ann = metrics_ann[\"F1\"]\n",
    "w_dt = metrics_tree[\"F1\"]\n",
    "w_knn = metrics_knn[\"F1\"]\n",
    "\n",
    "cv_scores = [w_ann, w_dt, w_knn]\n",
    "weights = cv_scores ./ sum(cv_scores)\n",
    "\n",
    "println(\"  Model weights:\")\n",
    "println(\"    ANN: $(round(weights[1]*100, digits=1))%\")\n",
    "println(\"    DT:  $(round(weights[2]*100, digits=1))%\")\n",
    "println(\"    kNN: $(round(weights[3]*100, digits=1))%\")\n",
    "\n",
    "weighted_predictions = weightedVoting(all_predictions, weights)\n",
    "cm_results_weighted = confusionMatrix(weighted_predictions, test_targets_str, classes_str; weighted=true)\n",
    "\n",
    "println(\"âœ… Weighted Voting Results:\")\n",
    "println(\"   F1: $(round(cm_results_weighted.aggregated.f1*100, digits=2))%\")\n",
    "println(\"   Acc: $(round(cm_results_weighted.accuracy*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08e8a98-7229-4f86-a84e-90297db63ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "ðŸ”¬ APPROACH 3: OVERSAMPLING\n",
      "######################################################################\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ EVALUATING APPROACH: Oversampling (incl. Ensembles)\n",
      "================================================================================\n",
      "\n",
      "[1/5] Testing ANNs...\n",
      "   âœ¨ Best ANN (CV): [256] - CV F1: 75.07%\n",
      "      âœ… ANN Results: F1=0.722\n",
      "\n",
      "[2/5] Testing SVMs...\n",
      "      âœ… SVM Results: F1=0.705\n",
      "\n",
      "[3/5] Testing Decision Trees...\n",
      "      âœ… DT Results: F1=0.714\n",
      "\n",
      "[4/5] Testing kNN...\n",
      "      âœ… kNN Results: F1=0.685\n",
      "\n",
      "[5/5] Testing Ensemble Methods...\n",
      "      âœ… Majority Voting: F1=0.711\n",
      "      âœ… Weighted Voting: F1=0.711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{String, Float64}} with 6 entries:\n",
       "  \"SVM\"            => Dict(\"Accuracy\"=>0.715, \"Sensitivity\"=>0.715, \"Specificitâ€¦\n",
       "  \"ANN\"            => Dict(\"Accuracy\"=>0.73, \"Sensitivity\"=>0.73, \"Specificity\"â€¦\n",
       "  \"DT\"             => Dict(\"Accuracy\"=>0.725, \"Sensitivity\"=>0.725, \"Specificitâ€¦\n",
       "  \"kNN\"            => Dict(\"Accuracy\"=>0.693333, \"Sensitivity\"=>0.693333, \"Specâ€¦\n",
       "  \"WeightedVoting\" => Dict(\"Accuracy\"=>0.721667, \"Sensitivity\"=>0.721667, \"Specâ€¦\n",
       "  \"MajorityVoting\" => Dict(\"Accuracy\"=>0.721667, \"Sensitivity\"=>0.721667, \"Specâ€¦"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#  APPROACH 2: OVERSAMPLING STRATEGY\n",
    "#  Description: Balance classes by duplicating minority samples instead of removing majority\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"#\"^70)\n",
    "println(\"ðŸ”¬ APPROACH 2: OVERSAMPLING\")\n",
    "println(\"#\"^70)\n",
    "\n",
    "# Function for Random Oversampling\n",
    "function random_oversampling(df, target_col)\n",
    "    classes = unique(df[!, target_col])\n",
    "    # Find count of majority class\n",
    "    max_count = maximum([sum(df[!, target_col] .== c) for c in classes])\n",
    "    \n",
    "    balanced_parts = []\n",
    "    for c in classes\n",
    "        df_class = df[df[!, target_col] .== c, :]\n",
    "        n_current = size(df_class, 1)\n",
    "        if n_current < max_count\n",
    "            # Oversample with replacement\n",
    "            ids = rand(1:n_current, max_count)\n",
    "            push!(balanced_parts, df_class[ids, :])\n",
    "        else\n",
    "            push!(balanced_parts, df_class)\n",
    "        end\n",
    "    end\n",
    "    return vcat(balanced_parts...)\n",
    "end\n",
    "\n",
    "# 1. Prepare Data (Oversampling on Training Data ONLY to prevent leakage)\n",
    "# Note: We use the raw training split created in Approach 1 section\n",
    "df_train_os = random_oversampling(df_train, \"Risk_Class\")\n",
    "\n",
    "# 2. Preprocess (Reuse existing function)\n",
    "df_train_os_proc, _ = preprocess_multiclass(df_train_os, \"Is Fraudulent\")\n",
    "input_cols_os = setdiff(names(df_train_os_proc), [\"Risk_Class\"])\n",
    "\n",
    "train_inputs_os = Matrix{Float32}(df_train_os_proc[:, input_cols_os])\n",
    "train_targets_os = Int.(df_train_os_proc.Risk_Class)\n",
    "\n",
    "# 3. Evaluate ALL models on this new dataset\n",
    "results_app3 = evaluate_approach(\"Oversampling\", train_inputs_os, train_targets_os, test_inputs, test_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86abe884-8b3b-4da4-a07d-4d734bcf1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "ðŸ”¬ APPROACH 4: PCA FEATURE EXTRACTION\n",
      "######################################################################\n",
      "   1. Fitting PCA on Training Set...\n",
      "   PCA Fit: Retaining 6 components (Variance covered: 95.75%)\n",
      "   2. Transforming Training Set...\n",
      "   3. Transforming Test Set (using Train projection)...\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ EVALUATING APPROACH: PCA (95% Variance) (incl. Ensembles)\n",
      "================================================================================\n",
      "\n",
      "[1/5] Testing ANNs...\n",
      "   âœ¨ Best ANN (CV): [64, 32] - CV F1: 71.83%\n",
      "      âœ… ANN Results: F1=0.696\n",
      "\n",
      "[2/5] Testing SVMs...\n",
      "      âœ… SVM Results: F1=0.61\n",
      "\n",
      "[3/5] Testing Decision Trees...\n",
      "      âœ… DT Results: F1=0.726\n",
      "\n",
      "[4/5] Testing kNN...\n",
      "      âœ… kNN Results: F1=0.713\n",
      "\n",
      "[5/5] Testing Ensemble Methods...\n",
      "      âœ… Majority Voting: F1=0.706\n",
      "      âœ… Weighted Voting: F1=0.732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{String, Float64}} with 6 entries:\n",
       "  \"SVM\"            => Dict(\"Accuracy\"=>0.631667, \"Sensitivity\"=>0.631667, \"Specâ€¦\n",
       "  \"ANN\"            => Dict(\"Accuracy\"=>0.706667, \"Sensitivity\"=>0.706667, \"Specâ€¦\n",
       "  \"DT\"             => Dict(\"Accuracy\"=>0.731667, \"Sensitivity\"=>0.731667, \"Specâ€¦\n",
       "  \"kNN\"            => Dict(\"Accuracy\"=>0.72, \"Sensitivity\"=>0.72, \"Specificity\"â€¦\n",
       "  \"WeightedVoting\" => Dict(\"Accuracy\"=>0.741667, \"Sensitivity\"=>0.741667, \"Specâ€¦\n",
       "  \"MajorityVoting\" => Dict(\"Accuracy\"=>0.715, \"Sensitivity\"=>0.715, \"Specificitâ€¦"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#  APPROACH 3: FEATURE EXTRACTION (PCA)\n",
    "#  Description: Reduce dimensionality using PCA before modeling.\n",
    "#  CRITICAL FIX: PCA matrix (W) is calculated on TRAIN and applied to TEST.\n",
    "# ============================================================================\n",
    "\n",
    "using LinearAlgebra # Required for PCA\n",
    "\n",
    "println(\"\\n\" * \"#\"^70)\n",
    "println(\"ðŸ”¬ APPROACH 3: PCA FEATURE EXTRACTION\")\n",
    "println(\"#\"^70)\n",
    "\n",
    "\"\"\"\n",
    "    fit_pca(data, variance_threshold)\n",
    "    \n",
    "Calculates the projection matrix W and normalization parameters based on the provided data (Training Set).\n",
    "Returns: (W, norm_params)\n",
    "\"\"\"\n",
    "function fit_pca(data, variance_threshold=0.95)\n",
    "    # 1. Calculate normalization parameters on TRAIN data\n",
    "    # We use ZeroMean normalization (Standardization) which is standard for PCA\n",
    "    norm_params = calculateZeroMeanNormalizationParameters(data)\n",
    "    \n",
    "    # 2. Standardize the data\n",
    "    data_std = normalizeZeroMean(data, norm_params)\n",
    "    \n",
    "    # 3. Covariance Matrix & Eigen decomposition\n",
    "    C = cov(data_std)\n",
    "    F = eigen(C)\n",
    "    \n",
    "    # 4. Sort eigenvalues (descending) and corresponding vectors\n",
    "    idx = sortperm(F.values, rev=true)\n",
    "    evals = F.values[idx]\n",
    "    evecs = F.vectors[:, idx]\n",
    "    \n",
    "    # 5. Select components to reach variance threshold\n",
    "    cum_var = cumsum(evals ./ sum(evals))\n",
    "    k = findfirst(x -> x >= variance_threshold, cum_var)\n",
    "    \n",
    "    if isnothing(k)\n",
    "        k = size(data, 2) # Keep all if threshold not reached\n",
    "    end\n",
    "    \n",
    "    println(\"   PCA Fit: Retaining $k components (Variance covered: $(round(cum_var[k]*100, digits=2))%)\")\n",
    "    \n",
    "    # 6. Construct Projection Matrix W\n",
    "    W = evecs[:, 1:k]\n",
    "    \n",
    "    return W, norm_params\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    transform_data_pca(data, W, norm_params)\n",
    "    \n",
    "Projects new data into the PCA space defined by W, using existing normalization parameters.\n",
    "\"\"\"\n",
    "function transform_data_pca(data, W, norm_params)\n",
    "    # 1. Normalize using the PARAMETERS from the Training Set (Critical!)\n",
    "    # Note: We assume normalizeZeroMean handles parameter application correctly\n",
    "    data_std = normalizeZeroMean(data, norm_params)\n",
    "    \n",
    "    # 2. Project into PCA space\n",
    "    return data_std * W\n",
    "end\n",
    "\n",
    "# --- EXECUTION STEPS ---\n",
    "\n",
    "# 1. Fit PCA model on Training Data ONLY\n",
    "# We calculate W (eigenvectors) and normalization stats from train_inputs\n",
    "println(\"   1. Fitting PCA on Training Set...\")\n",
    "pca_W, pca_norm_params = fit_pca(train_inputs, 0.95)\n",
    "\n",
    "# 2. Transform Training Data\n",
    "println(\"   2. Transforming Training Set...\")\n",
    "train_inputs_pca = transform_data_pca(train_inputs, pca_W, pca_norm_params)\n",
    "\n",
    "# 3. Transform Test Data\n",
    "# CRITICAL: We use the SAME W and norm_params calculated on Train\n",
    "println(\"   3. Transforming Test Set (using Train projection)...\")\n",
    "test_inputs_pca = transform_data_pca(test_inputs, pca_W, pca_norm_params)\n",
    "\n",
    "# 4. Evaluate Models on the new PCA-transformed space\n",
    "# We pass both the transformed train and transformed test sets\n",
    "results_app4 = evaluate_approach(\"PCA (95% Variance)\", \n",
    "                                 train_inputs_pca, train_targets, \n",
    "                                 test_inputs_pca, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b07d6-cb67-4a8a-a474-c52f02e3e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "ðŸ”¬ APPROACH 4: BINARY CLASSIFICATION (Original Fraud vs Not)\n",
      "######################################################################\n",
      "   Extracting original 'Is Fraudulent' labels from balanced dataframe...\n",
      "   Binary Train Distribution: Legit=1251, Fraud=1149\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#  APPROACH 4: BINARY CLASSIFICATION (ORIGINAL GROUND TRUTH)\n",
    "#  Description: Train directly on the original \"Is Fraudulent\" label.\n",
    "#  Why: Class 1 (Suspicious) contains both Frauds and Risky Legitimate ones.\n",
    "#       Merging 1 & 2 would confuse the model. We use the true binary label.\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"#\"^70)\n",
    "println(\"ðŸ”¬ APPROACH 4: BINARY CLASSIFICATION (Original Fraud vs Not)\")\n",
    "println(\"#\"^70)\n",
    "\n",
    "# 1. Extract the Original Binary Targets (0/1)\n",
    "# We go back to df_train/df_test because preprocessing dropped the target column\n",
    "println(\"   Extracting original 'Is Fraudulent' labels from balanced dataframe...\")\n",
    "\n",
    "# Ensure we align with the rows used in train_inputs (which come from df_train)\n",
    "train_targets_binary = Int.(df_train[!, \"Is Fraudulent\"])\n",
    "test_targets_binary  = Int.(df_test[!, \"Is Fraudulent\"])\n",
    "\n",
    "# Check distribution\n",
    "n_fraud = sum(train_targets_binary .== 1)\n",
    "n_legit = sum(train_targets_binary .== 0)\n",
    "println(\"   Binary Train Distribution: Legit=$n_legit, Fraud=$n_fraud\")\n",
    "\n",
    "# 2. Evaluate ALL models on Binary Targets\n",
    "# The inputs (train_inputs) remain the same (we keep the feature engineering like \"Is_Night\", etc.)\n",
    "# but we aim for the true binary target.\n",
    "results_app5 = evaluate_approach(\"Binary\", train_inputs, train_targets_binary, test_inputs, test_targets_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b598055-3d55-49a8-99bb-d0c01b1f10b2",
   "metadata": {},
   "source": [
    "# Final Results & Comparison\n",
    "\n",
    "Comprehensive comparison of all 6 approaches on the hold-out test set.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **F1 Score:** Harmonic mean of precision and recall\n",
    "- **Accuracy:** Overall correct predictions\n",
    "- **Per-Class Metrics:** Performance for each risk level\n",
    "\n",
    "**Key Question:** Which approach best balances overall performance with fraud detection capability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1900c344-0ae9-401b-bbed-4074445fb81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ðŸ† FINAL DETAILED RESULTS & COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“Œ Approach: 1. Undersampling\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Model              | Accuracy   | Sensitiv.  | Specific.  | AUC-ROC    | F1-Score  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "ANN                |    71.50%  |    71.50%  |    85.89%  |   0.5000     |    70.04%\n",
      "SVM                |    72.17%  |    72.17%  |    86.28%  |   0.5000     |    71.13%\n",
      "DT                 |    74.67%  |    74.67%  |    87.60%  |   0.5000     |    73.74%\n",
      "kNN                |    72.67%  |    72.67%  |    86.13%  |   0.5000     |    72.03%\n",
      "MajorityVoting     |    74.00%  |    74.00%  |    87.21%  |   0.5000     |    72.86%\n",
      "WeightedVoting     |    74.17%  |    74.17%  |    87.28%  |   0.5000     |    73.05%\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Œ Approach: 2. Oversampling\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Model              | Accuracy   | Sensitiv.  | Specific.  | AUC-ROC    | F1-Score  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "ANN                |    73.00%  |    73.00%  |    86.51%  |   0.5000     |    72.22%\n",
      "SVM                |    71.50%  |    71.50%  |    85.87%  |   0.5000     |    70.47%\n",
      "DT                 |    72.50%  |    72.50%  |    86.35%  |   0.5000     |    71.40%\n",
      "kNN                |    69.33%  |    69.33%  |    84.81%  |   0.5000     |    68.53%\n",
      "MajorityVoting     |    72.17%  |    72.17%  |    86.25%  |   0.5000     |    71.12%\n",
      "WeightedVoting     |    72.17%  |    72.17%  |    86.25%  |   0.5000     |    71.12%\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Œ Approach: 3. PCA Features\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Model              | Accuracy   | Sensitiv.  | Specific.  | AUC-ROC    | F1-Score  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "ANN                |    70.67%  |    70.67%  |    85.36%  |   0.5000     |    69.62%\n",
      "SVM                |    63.17%  |    63.17%  |    81.95%  |   0.5000     |    61.05%\n",
      "DT                 |    73.17%  |    73.17%  |    86.46%  |   0.5000     |    72.58%\n",
      "kNN                |    72.00%  |    72.00%  |    85.83%  |   0.5000     |    71.28%\n",
      "MajorityVoting     |    71.50%  |    71.50%  |    85.82%  |   0.5000     |    70.62%\n",
      "WeightedVoting     |    74.17%  |    74.17%  |    87.13%  |   0.5000     |    73.25%\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Œ Approach: 4. Binary Class.\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Model              | Accuracy   | Sensitiv.  | Specific.  | AUC-ROC    | F1-Score  \n",
      "-----------------------------------------------------------------------------------------------\n",
      "ANN                |    25.50%  |    31.34%  |    20.78%  |   0.8238     |    27.32%\n",
      "SVM                |    76.67%  |    75.75%  |    77.41%  |   0.5213     |    74.36%\n",
      "DT                 |    76.83%  |    76.87%  |    76.81%  |   0.5213     |    74.77%\n",
      "kNN                |    74.83%  |    67.16%  |    81.02%  |   0.5213     |    70.45%\n",
      "MajorityVoting     |    77.00%  |    76.49%  |    77.41%  |   0.5213     |    74.82%\n",
      "WeightedVoting     |    77.00%  |    76.49%  |    77.41%  |   0.5213     |    74.82%\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ OVERALL BEST PERFORMANCE\n",
      "   Approach: 4. Binary Class.\n",
      "   Model:    WeightedVoting\n",
      "   F1 Score: 74.82%\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ PROJECT SUMMARY:\n",
      "  âœ… Tested 4 distinct Data Approaches (Under, Over, PCA, Binary)\n",
      "  âœ… Evaluated 4 ML Algorithms + Ensembles for EACH approach\n",
      "  âœ… Data Leakage prevention implemented (Strict Train/Test separation)\n",
      "  âœ… Full metrics comparison (Accuracy, Sensitivity, Specificity, AUC, F1)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#  FINAL COMPARISON SUMMARY - FULL METRICS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^100)\n",
    "println(\"ðŸ† FINAL DETAILED RESULTS & COMPARISON\")\n",
    "println(\"=\"^100)\n",
    "\n",
    "using Printf\n",
    "\n",
    "# --- 1. FUNZIONE DI STAMPA TABELLA ---\n",
    "function print_detailed_table(approach_name, res_dict)\n",
    "    println(\"\\nðŸ“Œ Approach: $approach_name\")\n",
    "    println(\"-\"^95)\n",
    "    @printf(\"%-18s | %-10s | %-10s | %-10s | %-10s | %-10s\\n\", \n",
    "            \"Model\", \"Accuracy\", \"Sensitiv.\", \"Specific.\", \"AUC-ROC\", \"F1-Score\")\n",
    "    println(\"-\"^95)\n",
    "    \n",
    "    # Ordine di stampa preferito\n",
    "    model_order = [\"ANN\", \"SVM\", \"DT\", \"kNN\", \"MajorityVoting\", \"WeightedVoting\"]\n",
    "    \n",
    "    # Trova quali modelli sono presenti nel dizionario\n",
    "    present_models = filter(m -> haskey(res_dict, m), model_order)\n",
    "    \n",
    "    for model in present_models\n",
    "        m = res_dict[model]\n",
    "        # Gestione sicura dei valori (se mancano mette 0.0)\n",
    "        acc  = get(m, \"Accuracy\", 0.0) * 100\n",
    "        sens = get(m, \"Sensitivity\", 0.0) * 100\n",
    "        spec = get(m, \"Specificity\", 0.0) * 100\n",
    "        auc  = get(m, \"AUC\", 0.0)\n",
    "        f1   = get(m, \"F1\", 0.0) * 100\n",
    "        \n",
    "        @printf(\"%-18s | %8.2f%%  | %8.2f%%  | %8.2f%%  | %8.4f     | %8.2f%%\\n\", \n",
    "                model, acc, sens, spec, auc, f1)\n",
    "    end\n",
    "    println(\"-\"^95)\n",
    "end\n",
    "\n",
    "# --- 2. RECUPERO E NORMALIZZAZIONE DATI APPROCCIO 1 ---\n",
    "# Creiamo un dizionario unico per l'Approccio 1 che includa anche gli Ensemble\n",
    "results_app1_full = Dict{String, Dict{String, Float64}}()\n",
    "\n",
    "# Funzione helper per estrarre metriche da oggetti ConfusionMatrix (usati nei vecchi ensemble)\n",
    "function extract_from_cm(cm_obj)\n",
    "    return Dict(\n",
    "        \"Accuracy\"    => cm_obj.accuracy,\n",
    "        \"Sensitivity\" => cm_obj.aggregated.sensitivity,\n",
    "        \"Specificity\" => cm_obj.aggregated.specificity,\n",
    "        \"F1\"          => cm_obj.aggregated.f1,\n",
    "        \"AUC\"         => 0.5 # Placeholder per ensemble senza probabilitÃ \n",
    "    )\n",
    "end\n",
    "\n",
    "# Aggiungi i modelli singoli (se esistono le variabili metrics_...)\n",
    "if isdefined(Main, :metrics_ann) results_app1_full[\"ANN\"] = metrics_ann end\n",
    "if isdefined(Main, :metrics_svm) results_app1_full[\"SVM\"] = metrics_svm end\n",
    "if isdefined(Main, :metrics_tree) results_app1_full[\"DT\"] = metrics_tree end\n",
    "if isdefined(Main, :metrics_knn) results_app1_full[\"kNN\"] = metrics_knn end\n",
    "\n",
    "# Aggiungi gli Ensemble (convertendo gli oggetti cm_results_... in dizionari)\n",
    "if isdefined(Main, :cm_results_majority) \n",
    "    results_app1_full[\"MajorityVoting\"] = extract_from_cm(cm_results_majority) \n",
    "end\n",
    "if isdefined(Main, :cm_results_weighted) \n",
    "    results_app1_full[\"WeightedVoting\"] = extract_from_cm(cm_results_weighted) \n",
    "end\n",
    "\n",
    "# --- 3. STAMPA DELLE TABELLE ---\n",
    "\n",
    "# Lista di tutti gli approcci potenziali\n",
    "all_approaches = [\n",
    "    (\"1. Undersampling\", results_app1_full),\n",
    "    (\"2. Oversampling\", isdefined(Main, :results_app3) ? results_app3 : Dict()),\n",
    "    (\"3. PCA Features\", isdefined(Main, :results_app4) ? results_app4 : Dict()),\n",
    "    (\"4. Binary Class.\", isdefined(Main, :results_app5) ? results_app5 : Dict())\n",
    "]\n",
    "\n",
    "for (name, data) in all_approaches\n",
    "    if !isempty(data)\n",
    "        print_detailed_table(name, data)\n",
    "    else\n",
    "        println(\"\\nðŸ“Œ Approach: $name (Not Executed)\")\n",
    "    end\n",
    "end\n",
    "\n",
    "# --- 4. CALCOLO VINCITORE ASSOLUTO ---\n",
    "best_f1 = -1.0\n",
    "best_model_name = \"None\"\n",
    "best_approach_name = \"None\"\n",
    "\n",
    "for (app_name, data) in all_approaches\n",
    "    for (model, metrics) in data\n",
    "        if get(metrics, \"F1\", 0.0) > best_f1\n",
    "            global best_f1 = metrics[\"F1\"]\n",
    "            global best_model_name = model\n",
    "            global best_approach_name = app_name\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"ðŸŽ¯ OVERALL BEST PERFORMANCE\")\n",
    "println(\"   Approach: $best_approach_name\")\n",
    "println(\"   Model:    $best_model_name\")\n",
    "println(\"   F1 Score: $(round(best_f1*100, digits=2))%\")\n",
    "println(\"=\"^80)\n",
    "\n",
    "println(\"\\nðŸ“‹ PROJECT SUMMARY:\")\n",
    "println(\"  âœ… Tested 4 distinct Data Approaches (Under, Over, PCA, Binary)\")\n",
    "println(\"  âœ… Evaluated 4 ML Algorithms + Ensembles for EACH approach\")\n",
    "println(\"  âœ… Data Leakage prevention implemented (Strict Train/Test separation)\")\n",
    "println(\"  âœ… Full metrics comparison (Accuracy, Sensitivity, Specificity, AUC, F1)\")\n",
    "println(\"=\"^80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a56b09-57cc-47eb-9367-07d7d34d42cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
