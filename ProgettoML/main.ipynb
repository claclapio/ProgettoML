{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee6521e-c28b-40d4-97b5-b57a1498f141",
   "metadata": {},
   "source": [
    "# Fraud Detection - Multiclass Classification\n",
    "## Machine Learning I - Group Project\n",
    "\n",
    "**Objective:** E-commerce fraud detection using multiclass classification (Legitimate, Suspicious, Fraudulent)\n",
    "\n",
    "**Authors:** [Your Names]  \n",
    "**Date:** December 2024\n",
    "\n",
    "---\n",
    "\n",
    "### Methodology:\n",
    "- **4 ML Algorithms:** ANNs (8 topologies), SVMs (10 configs), Decision Trees (7 depths), kNN (6 k values)\n",
    "- **Ensemble Methods:** Majority Voting + Weighted Voting\n",
    "- **Dataset:** 1.5M e-commerce transactions â†’ 3-class risk assessment\n",
    "- **Cross-Validation:** 3-fold stratified on training set\n",
    "- **Evaluation:** Hold-out test set (20%)\n",
    "\n",
    "### Code Organization:\n",
    "```\n",
    "/project/\n",
    "â”œâ”€â”€ main.jl                    â† This file (executable from top to bottom)\n",
    "â”œâ”€â”€ /utils/\n",
    "â”‚   â”œâ”€â”€ utils.jl              â† Course utilities (includes modelCrossValidation)\n",
    "â”‚   â””â”€â”€ preprocessing.jl      â† Custom preprocessing functions\n",
    "â””â”€â”€ /datasets/\n",
    "    â””â”€â”€ Fraudulent_E-Commerce_Transaction_Data_merge.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78a1c4-2880-4800-923c-b09809f18793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    SETUP & IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "using Random\n",
    "Random.seed!(42)\n",
    "\n",
    "# Load packages\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Statistics\n",
    "using Dates\n",
    "using StatsBase\n",
    "\n",
    "println(\"âœ… Packages loaded!\")\n",
    "\n",
    "# Load course utilities\n",
    "include(\"utils/utils.jl\")\n",
    "println(\"âœ… Course utilities loaded (includes modelCrossValidation, confusionMatrix, etc.)\")\n",
    "\n",
    "# Load custom preprocessing\n",
    "include(\"utils/preprocessing.jl\")\n",
    "using .PreprocessingUtils\n",
    "println(\"âœ… Custom preprocessing utilities loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47e1f2-2b47-442c-8c91-0bc5ab2dc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  HELPER FUNCTION: EVALUATE ALL MODELS FOR A GIVEN APPROACH\n",
    "# ============================================================================\n",
    "# This function ensures we strictly follow the PDF requirement:\n",
    "# \"Each approach must test the four ML techniques covered in class\"\n",
    "# and tests multiple configurations as requested.\n",
    "\n",
    "function evaluate_approach(approach_name, train_inputs, train_targets; cv_folds=3)\n",
    "    println(\"\\n\" * \"=\"^70)\n",
    "    println(\"ðŸš€ EVALUATING APPROACH: $approach_name\")\n",
    "    println(\"=\"^70)\n",
    "    \n",
    "    # Generate CV indices for this specific dataset/target\n",
    "    cv_indices = crossvalidation(train_targets, cv_folds)\n",
    "    \n",
    "    best_results = Dict()\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1. Artificial Neural Networks (ANNs)\n",
    "    # Requirement: Test at least 8 architectures (1-2 hidden layers)\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[1/4] Testing ANNs (8 Architectures)...\")\n",
    "    ann_topologies = [\n",
    "        [256], [128], [64], [32],       # 1 Hidden Layer\n",
    "        [256, 128], [128, 64], [64, 32], [96, 48] # 2 Hidden Layers\n",
    "    ]\n",
    "    \n",
    "    best_f1_ann = 0.0\n",
    "    best_topo_ann = []\n",
    "    \n",
    "    for topology in ann_topologies\n",
    "        hyperparams = Dict(\n",
    "            \"topology\" => topology,\n",
    "            \"learningRate\" => 0.003,\n",
    "            \"validationRatio\" => 0.1,\n",
    "            \"numExecutions\" => 1, # Increase if needed\n",
    "            \"maxEpochs\" => 800,\n",
    "            \"maxEpochsVal\" => 25\n",
    "        )\n",
    "        \n",
    "        # Using the course provided function\n",
    "        results = modelCrossValidation(:ANN, hyperparams, (train_inputs, train_targets), cv_indices)\n",
    "        f1 = results[7][1] # extract mean F1\n",
    "        \n",
    "        if f1 > best_f1_ann\n",
    "            best_f1_ann = f1\n",
    "            best_topo_ann = topology\n",
    "        end\n",
    "    end\n",
    "    println(\"   âœ¨ Best ANN: $best_topo_ann - F1: $(round(best_f1_ann*100, digits=2))%\")\n",
    "    best_results[\"ANN\"] = best_f1_ann\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2. Support Vector Machines (SVMs)\n",
    "    # Requirement: Test at least 8 configurations\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[2/4] Testing SVMs (Configs)...\")\n",
    "    svm_configs = [\n",
    "        (\"linear\", 1.0, 0.1, 3), (\"linear\", 10.0, 0.1, 3),\n",
    "        (\"rbf\", 1.0, 0.125, 3), (\"rbf\", 10.0, 0.125, 3), (\"rbf\", 100.0, 0.125, 3),\n",
    "        (\"poly\", 1.0, 0.1, 2), (\"poly\", 10.0, 0.1, 2), (\"poly\", 1.0, 0.1, 3)\n",
    "    ]\n",
    "    \n",
    "    best_f1_svm = 0.0\n",
    "    best_cfg_svm = \"\"\n",
    "    \n",
    "    for (kernel, C, gamma, degree) in svm_configs\n",
    "        hyperparams = Dict(\"kernel\"=>kernel, \"C\"=>C, \"gamma\"=>gamma, \"degree\"=>degree)\n",
    "        results = modelCrossValidation(:SVC, hyperparams, (train_inputs, train_targets), cv_indices)\n",
    "        f1 = results[7][1]\n",
    "        \n",
    "        if f1 > best_f1_svm\n",
    "            best_f1_svm = f1\n",
    "            best_cfg_svm = \"$kernel C=$C\"\n",
    "        end\n",
    "    end\n",
    "    println(\"   âœ¨ Best SVM: $best_cfg_svm - F1: $(round(best_f1_svm*100, digits=2))%\")\n",
    "    best_results[\"SVM\"] = best_f1_svm\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3. Decision Trees\n",
    "    # Requirement: Test at least 6 different maximum depths\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[3/4] Testing Decision Trees (6 Depths)...\")\n",
    "    depths = [3, 5, 7, 10, 15, 20]\n",
    "    \n",
    "    best_f1_dt = 0.0\n",
    "    best_depth_dt = 0\n",
    "    \n",
    "    for depth in depths\n",
    "        hyperparams = Dict(\"max_depth\" => depth)\n",
    "        results = modelCrossValidation(:DecisionTreeClassifier, hyperparams, (train_inputs, train_targets), cv_indices)\n",
    "        f1 = results[7][1]\n",
    "        \n",
    "        if f1 > best_f1_dt\n",
    "            best_f1_dt = f1\n",
    "            best_depth_dt = depth\n",
    "        end\n",
    "    end\n",
    "    println(\"   âœ¨ Best Tree: Depth=$best_depth_dt - F1: $(round(best_f1_dt*100, digits=2))%\")\n",
    "    best_results[\"DT\"] = best_f1_dt\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4. k-Nearest Neighbors (kNN)\n",
    "    # Requirement: Test at least 6 different values of k\n",
    "    # ------------------------------------------------------------------------\n",
    "    println(\"\\n[4/4] Testing kNN (6 k-values)...\")\n",
    "    k_values = [1, 3, 5, 7, 10, 15]\n",
    "    \n",
    "    best_f1_knn = 0.0\n",
    "    best_k_knn = 0\n",
    "    \n",
    "    for k in k_values\n",
    "        hyperparams = Dict(\"n_neighbors\" => k)\n",
    "        results = modelCrossValidation(:KNeighborsClassifier, hyperparams, (train_inputs, train_targets), cv_indices)\n",
    "        f1 = results[7][1]\n",
    "        \n",
    "        if f1 > best_f1_knn\n",
    "            best_f1_knn = f1\n",
    "            best_k_knn = k\n",
    "        end\n",
    "    end\n",
    "    println(\"   âœ¨ Best kNN: k=$best_k_knn - F1: $(round(best_f1_knn*100, digits=2))%\")\n",
    "    best_results[\"kNN\"] = best_f1_knn\n",
    "    \n",
    "    return best_results\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59226d32-d227-4e1f-961f-607df6634244",
   "metadata": {},
   "source": [
    "## 1. Data Loading & 3-Class Target Creation\n",
    "\n",
    "**Dataset:** Fraudulent E-Commerce Transaction Data (1.5M transactions)\n",
    "\n",
    "**Target Creation Strategy:**\n",
    "- Original: Binary fraud labels (fraud vs non-fraud)\n",
    "- **Our approach:** 3-class risk assessment based on multiple signals:\n",
    "  1. **Time Risk:** Night transactions (0-5am, 11pm)\n",
    "  2. **Amount Risk:** High-value transactions (>90th percentile)\n",
    "  3. **Account Age Risk:** New accounts (<30 days)\n",
    "\n",
    "**Class Mapping:**\n",
    "- **Class 0 (LEGITTIMO):** Low-risk, legitimate transactions\n",
    "- **Class 1 (SOSPETTO):** Borderline cases requiring manual review\n",
    "- **Class 2 (FRAUDOLENTO):** High-risk fraudulent transactions\n",
    "\n",
    "**Justification:** This approach allows for graduated risk assessment, enabling businesses to:\n",
    "- Automatically approve low-risk transactions (Class 0)\n",
    "- Flag suspicious cases for manual review (Class 1)\n",
    "- Immediately block high-risk frauds (Class 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de582c0d-ce18-4b53-ae3b-8bbc64b19877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#              DATA LOADING & 3-CLASS TARGET CREATION\n",
    "# ============================================================================\n",
    "\n",
    "const DATA_PATH = \"datasets/Fraudulent_E-Commerce_Transaction_Data_merge.csv\"\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ“‚ LOADING DATA\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "df = CSV.read(DATA_PATH, DataFrame)\n",
    "target_col = \"Is Fraudulent\"\n",
    "\n",
    "println(\"Original dataset size: $(size(df))\")\n",
    "println(\"Original fraud distribution:\")\n",
    "println(\"  Non-fraud: $(sum(df[!, target_col] .== 0))\")\n",
    "println(\"  Fraud:     $(sum(df[!, target_col] .== 1))\")\n",
    "\n",
    "# Create 3-class target\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸŽ¯ CREATING 3-CLASS TARGET\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "df_with_classes = create_risk_classes(df, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199d34c-9167-4dfd-92e2-d4fa2175fb37",
   "metadata": {},
   "source": [
    "## 2. Class Balancing & Train/Test Split\n",
    "\n",
    "**Challenge:** Highly imbalanced dataset (90% Legitimate, 8.6% Suspicious, 1.4% Fraudulent)\n",
    "\n",
    "**Solution:** Undersample majority classes to match minority class (20,654 samples per class)\n",
    "\n",
    "**Train/Test Split:**\n",
    "- **80% Training** (49,569 samples) - used for cross-validation and model selection\n",
    "- **20% Test** (12,393 samples) - held out for final evaluation\n",
    "\n",
    "**Critical:** Test set is NEVER used during training or model selection to prevent data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75628c65-341c-40c6-8e51-ddcc02957a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#          CLASS BALANCING & TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"âœ… TRAIN/TEST SPLIT (80% Train / 20% Test)\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "# Balance classes\n",
    "class_0 = df_with_classes[df_with_classes.Risk_Class .== 0, :]\n",
    "class_1 = df_with_classes[df_with_classes.Risk_Class .== 1, :]\n",
    "class_2 = df_with_classes[df_with_classes.Risk_Class .== 2, :]\n",
    "\n",
    "n_min = minimum([size(class_0, 1), size(class_1, 1), size(class_2, 1)])\n",
    "n_target = min(n_min, 40000)\n",
    "\n",
    "println(\"\\nðŸ”„ Balancing dataset...\")\n",
    "println(\"  Samples per class: $n_target\")\n",
    "\n",
    "class_0_sample = class_0[shuffle(1:size(class_0, 1))[1:n_target], :]\n",
    "class_1_sample = class_1[shuffle(1:size(class_1, 1))[1:n_target], :]\n",
    "class_2_sample = class_2[shuffle(1:size(class_2, 1))[1:n_target], :]\n",
    "\n",
    "df_balanced = vcat(class_0_sample, class_1_sample, class_2_sample)\n",
    "df_balanced = df_balanced[shuffle(1:size(df_balanced, 1)), :]\n",
    "\n",
    "println(\"  Balanced dataset size: $(size(df_balanced))\")\n",
    "\n",
    "# Split Train/Test BEFORE preprocessing (critical!)\n",
    "n_total = size(df_balanced, 1)\n",
    "n_train = floor(Int, n_total * 0.80)\n",
    "n_test = n_total - n_train\n",
    "\n",
    "all_indices = shuffle(1:n_total)\n",
    "train_indices = all_indices[1:n_train]\n",
    "test_indices = all_indices[n_train+1:end]\n",
    "\n",
    "df_train = df_balanced[train_indices, :]\n",
    "df_test = df_balanced[test_indices, :]\n",
    "\n",
    "println(\"\\nðŸ“Š Split Summary:\")\n",
    "println(\"  Total samples:     $n_total\")\n",
    "println(\"  Training set:      $n_train (80%)\")\n",
    "println(\"  Test set:          $n_test (20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c850a-1ee0-47f8-8ba8-5ffbb6db6706",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Feature Engineering\n",
    "\n",
    "**Steps:**\n",
    "1. **Time Features:** Extract hour, create night flag (hour < 6)\n",
    "2. **Feature Engineering:**\n",
    "   - `Amount_per_AccountAge`: Transaction amount relative to account maturity\n",
    "   - `High_Value_Flag`: Transactions above 95th percentile\n",
    "   - `New_Account_Flag`: Accounts younger than 30 days\n",
    "3. **Missing Value Imputation:** Median imputation\n",
    "4. **Feature Selection:** Drop IDs, addresses, categorical features â†’ **8 numerical features**\n",
    "5. **Normalization:** Min-Max [0,1] using training set parameters only\n",
    "\n",
    "**Final Features (8):**\n",
    "- Transaction Amount\n",
    "- Account Age Days  \n",
    "- Transaction_Hour\n",
    "- Is_Night\n",
    "- Amount_per_AccountAge\n",
    "- High_Value_Flag\n",
    "- New_Account_Flag\n",
    "- (1 more from preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a8af7-0d2a-4e4a-a9f3-73416da93e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                    PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\nðŸ”§ Preprocessing train and test sets...\")\n",
    "df_train_processed = preprocess_multiclass(df_train, target_col)\n",
    "df_test_processed = preprocess_multiclass(df_test, target_col)\n",
    "\n",
    "input_cols = setdiff(names(df_train_processed), [\"Risk_Class\"])\n",
    "train_inputs = Matrix{Float32}(df_train_processed[:, input_cols])\n",
    "train_targets = Int.(df_train_processed.Risk_Class)\n",
    "\n",
    "test_inputs = Matrix{Float32}(df_test_processed[:, input_cols])\n",
    "test_targets = Int.(df_test_processed.Risk_Class)\n",
    "\n",
    "println(\"\\nðŸ“Š Preprocessed Data:\")\n",
    "println(\"  Features: $(length(input_cols))\")\n",
    "println(\"  Train samples: $(size(train_inputs, 1))\")\n",
    "println(\"  Test samples: $(size(test_inputs, 1))\")\n",
    "println(\"\\n  Feature names: $input_cols\")\n",
    "\n",
    "# Create cross-validation indices (3-fold stratified)\n",
    "k_folds = 3\n",
    "cv_indices = crossvalidation(train_targets, k_folds)\n",
    "println(\"\\nâœ… Cross-validation indices created ($k_folds folds, stratified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e7d2f-dda1-4c71-8050-1ac38fd37097",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: Artificial Neural Networks (ANNs)\n",
    "\n",
    "**Configuration:**\n",
    "- **Topologies tested:** 8 architectures (1-4 hidden layers)\n",
    "- **Activation:** ReLU (hidden layers), Softmax (output)\n",
    "- **Optimizer:** Adam (learning rate: 0.003)\n",
    "- **Loss:** Cross-entropy\n",
    "- **Regularization:** Early stopping (patience: 25 epochs)\n",
    "- **Validation:** 10% of training set\n",
    "- **Executions:** 1 per topology (for speed; can increase for stability)\n",
    "\n",
    "**Architectures:**\n",
    "1. `[256]` - Large\n",
    "2. `[128]` - Medium\n",
    "3. `[64]` - Small\n",
    "4. `[32]` - Tiny\n",
    "5. `[256, 128]` - Large 2-layer\n",
    "6. `[128, 64]` - Medium 2-layer\n",
    "7. `[64, 32]` - Small 2-layer\n",
    "8. `[96, 48]` - Alternative 2-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23091362-9363-4891-a1d9-46ffb4d9a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#        EXPERIMENT 1: ARTIFICIAL NEURAL NETWORKS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 1: ARTIFICIAL NEURAL NETWORKS\")\n",
    "println(\"Testing 8 ANN Topologies\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "topologies_to_test = [\n",
    "    [256],            # 1. 1 hidden layer - Large\n",
    "    [128],            # 2. 1 hidden layer - Medium\n",
    "    [64],             # 3. 1 hidden layer - Small\n",
    "    [32],             # 4. 1 hidden layer - Tiny\n",
    "    [256, 128],       # 5. 2 hidden layers - Large\n",
    "    [128, 64],        # 6. 2 hidden layers - Medium\n",
    "    [64, 32],         # 7. 2 hidden layers - Small\n",
    "    [96, 48]          # 8. 2 hidden layers - Alternative\n",
    "]\n",
    "\n",
    "ann_results = []\n",
    "\n",
    "for (i, topology) in enumerate(topologies_to_test)\n",
    "    println(\"\\n[$i/8] Testing topology: $topology\")\n",
    "    \n",
    "    hyperparams = Dict(\n",
    "        \"topology\" => topology,\n",
    "        \"learningRate\" => 0.003,\n",
    "        \"validationRatio\" => 0.1,\n",
    "        \"numExecutions\" => 1,\n",
    "        \"maxEpochs\" => 800,\n",
    "        \"maxEpochsVal\" => 25\n",
    "    )\n",
    "    \n",
    "    # Use modelCrossValidation from utils.jl\n",
    "    results = modelCrossValidation(\n",
    "        :ANN,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    \n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(ann_results, (topology, f1_stats[1], results))\n",
    "end\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_ann_results = sort(ann_results, by=x->x[2], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† ANN Results Ranking (by F1 Score):\")\n",
    "println(\"-\"^70)\n",
    "for (i, (topo, f1, _)) in enumerate(sorted_ann_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. $topo - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_topology_ann = sorted_ann_results[1][1]\n",
    "best_f1_ann = sorted_ann_results[1][2]\n",
    "println(\"\\nâœ¨ Best ANN: $best_topology_ann (CV F1: $(round(best_f1_ann*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46b960-4937-483a-bcc4-4a02baf19215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final ANN on full training set and evaluate on test set\n",
    "println(\"\\nðŸš€ Training final ANN on full training set...\")\n",
    "\n",
    "# 1. Definisci le classi in modo univoco e ordinato (FONDAMENTALE)\n",
    "classes = sort(unique(train_targets)) \n",
    "\n",
    "# 2. Usa queste classi per entrambi gli encoding\n",
    "train_targets_onehot = oneHotEncoding(train_targets, classes)\n",
    "test_targets_onehot = oneHotEncoding(test_targets, classes)\n",
    "\n",
    "# 3. Normalizzazione\n",
    "normParams_ann = calculateMinMaxNormalizationParameters(train_inputs)\n",
    "train_inputs_norm = normalizeMinMax(train_inputs, normParams_ann)\n",
    "test_inputs_norm = normalizeMinMax(test_inputs, normParams_ann)\n",
    "\n",
    "# 4. Creazione validation split\n",
    "N_train = size(train_inputs_norm, 1)\n",
    "(train_idx, val_idx) = holdOut(N_train, 0.1)\n",
    "\n",
    "# 5. Training (con la funzione interna corretta \"_\")\n",
    "final_ann, _ = _trainClassANN(best_topology_ann,\n",
    "    (train_inputs_norm[train_idx, :], train_targets_onehot[train_idx, :]),\n",
    "    validationDataset=(train_inputs_norm[val_idx, :], train_targets_onehot[val_idx, :]),\n",
    "    testDataset=(test_inputs_norm, test_targets_onehot),\n",
    "    maxEpochs=800,\n",
    "    learningRate=0.003,\n",
    "    maxEpochsVal=25)\n",
    "\n",
    "# Predict on test set\n",
    "test_outputs_ann = final_ann(test_inputs_norm')'\n",
    "\n",
    "# Calculate metrics\n",
    "cm_results_ann = confusionMatrix(test_outputs_ann, test_targets_onehot; weighted=true)\n",
    "\n",
    "println(\"\\nðŸ“Š ANN TEST SET RESULTS:\")\n",
    "println(\"=\"^70)\n",
    "println(\"Accuracy:  $(round(cm_results_ann.accuracy*100, digits=2))%\")\n",
    "println(\"F1 Score:  $(round(cm_results_ann.aggregated.f1*100, digits=2))%\")\n",
    "println(\"\\nConfusion Matrix:\")\n",
    "printConfusionMatrix(test_outputs_ann, test_targets_onehot; weighted=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50702451-b1b3-4ccd-b022-2432a2e0e76d",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2: Support Vector Machines (SVMs)\n",
    "\n",
    "**Configuration:**\n",
    "- **10 configurations tested**\n",
    "- **Kernels:** Linear, RBF, Polynomial\n",
    "- **Hyperparameter C:** 0.1, 1.0, 10.0\n",
    "- **Gamma (RBF):** auto (1/n_features), 0.1\n",
    "- **Degree (Polynomial):** 2, 3\n",
    "\n",
    "**Configurations:**\n",
    "1-3. Linear (C = 0.1, 1.0, 10.0)\n",
    "4-6. RBF with auto gamma (C = 0.1, 1.0, 10.0)\n",
    "7. RBF with Î³=0.1 (C = 1.0)\n",
    "8-10. Polynomial degree 2, 3 (C = 1.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ff0a3-d803-4e0c-9e27-66b359bb067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#        EXPERIMENT 2: SUPPORT VECTOR MACHINES\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 2: SUPPORT VECTOR MACHINES\")\n",
    "println(\"Testing 10 SVM Configurations\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "svm_configs = [\n",
    "    (\"linear\", 0.1, 0.125, 3, \"Linear C=0.1\"),\n",
    "    (\"linear\", 1.0, 0.125, 3, \"Linear C=1.0\"),\n",
    "    (\"linear\", 10.0, 0.125, 3, \"Linear C=10.0\"),\n",
    "    (\"rbf\", 0.1, 0.125, 3, \"RBF C=0.1 Î³=auto\"),\n",
    "    (\"rbf\", 1.0, 0.125, 3, \"RBF C=1.0 Î³=auto\"),\n",
    "    (\"rbf\", 10.0, 0.125, 3, \"RBF C=10.0 Î³=auto\"),\n",
    "    (\"rbf\", 1.0, 0.1, 3, \"RBF C=1.0 Î³=0.1\"),\n",
    "    (\"poly\", 1.0, 0.125, 2, \"Poly C=1.0 deg=2\"),\n",
    "    (\"poly\", 1.0, 0.125, 3, \"Poly C=1.0 deg=3\"),\n",
    "    (\"poly\", 10.0, 0.125, 2, \"Poly C=10.0 deg=2\")\n",
    "]\n",
    "\n",
    "svm_results = []\n",
    "\n",
    "for (i, (kernel, C, gamma, degree, desc)) in enumerate(svm_configs)\n",
    "    println(\"\\n[$i/10] Testing: $desc\")\n",
    "    \n",
    "    hyperparams = Dict(\n",
    "        \"kernel\" => kernel,\n",
    "        \"C\" => C,\n",
    "        \"gamma\" => gamma,\n",
    "        \"degree\" => degree\n",
    "    )\n",
    "    \n",
    "    results = modelCrossValidation(\n",
    "        :SVC,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(svm_results, (desc, f1_stats[1], kernel, C, gamma, degree, results))\n",
    "end\n",
    "\n",
    "sorted_svm_results = sort(svm_results, by=x->x[2], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† SVM Results Ranking:\")\n",
    "println(\"-\"^70)\n",
    "for (i, (desc, f1, _, _, _, _, _)) in enumerate(sorted_svm_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. $desc - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_svm = sorted_svm_results[1]\n",
    "best_desc_svm, best_f1_svm, best_kernel_svm, best_C_svm, best_gamma_svm, best_degree_svm = best_svm[1:6]\n",
    "println(\"\\nâœ¨ Best SVM: $best_desc_svm (CV F1: $(round(best_f1_svm*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ceeb5f-483f-4bd8-8d31-7293fa617911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final SVM and evaluate on test set\n",
    "println(\"\\nðŸš€ Training final SVM on full training set...\")\n",
    "\n",
    "hyperparams_svm_final = Dict(\n",
    "    \"kernel\" => best_kernel_svm,\n",
    "    \"C\" => best_C_svm,\n",
    "    \"gamma\" => best_gamma_svm,\n",
    "    \"degree\" => best_degree_svm\n",
    ")\n",
    "\n",
    "# For final model, we train on full train set and predict on test set\n",
    "# Using MLJ directly for final model\n",
    "using MLJ\n",
    "SVMClassifier = @load SVC pkg=LIBSVM\n",
    "\n",
    "# Normalize\n",
    "train_inputs_norm_svm = normalizeMinMax(train_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "test_inputs_norm_svm = normalizeMinMax(test_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "\n",
    "# Convert to strings for MLJ\n",
    "train_targets_str = string.(train_targets)\n",
    "test_targets_str = string.(test_targets)\n",
    "\n",
    "# Set kernel\n",
    "if best_kernel_svm == \"linear\"\n",
    "    kernel_func = LIBSVM.Kernel.Linear\n",
    "elseif best_kernel_svm == \"poly\"\n",
    "    kernel_func = LIBSVM.Kernel.Polynomial\n",
    "else\n",
    "    kernel_func = LIBSVM.Kernel.RadialBasis\n",
    "end\n",
    "\n",
    "model_svm = SVMClassifier(\n",
    "    kernel=kernel_func,\n",
    "    cost=best_C_svm,\n",
    "    gamma=best_gamma_svm,\n",
    "    degree=Int32(best_degree_svm)\n",
    ")\n",
    "\n",
    "mach_svm = machine(model_svm, MLJ.table(train_inputs_norm_svm), categorical(train_targets_str))\n",
    "MLJ.fit!(mach_svm, verbosity=0)\n",
    "\n",
    "# Predict\n",
    "svm_predictions = MLJ.predict(mach_svm, MLJ.table(test_inputs_norm_svm))\n",
    "classes = sort(unique(test_targets_str))\n",
    "cm_results_svm = confusionMatrix(svm_predictions, test_targets_str, classes; weighted=true)\n",
    "\n",
    "println(\"\\nðŸ“Š SVM TEST SET RESULTS:\")\n",
    "println(\"=\"^70)\n",
    "println(\"Accuracy:  $(round(cm_results_svm.accuracy*100, digits=2))%\")\n",
    "println(\"F1 Score:  $(round(cm_results_svm.aggregated.f1*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fb632-2fd0-41d4-91d4-9a075baeb3c9",
   "metadata": {},
   "source": [
    "# EXPERIMENT 3: Decision Trees\n",
    "\n",
    "**Configuration:**\n",
    "- **7 maximum depths tested:** 3, 5, 7, 10, 15, 20, unlimited\n",
    "- **Splitting criterion:** Gini impurity\n",
    "- **Min samples split:** 2\n",
    "- **Random seed:** 42 (for reproducibility)\n",
    "\n",
    "**Advantages:**\n",
    "- Interpretable (can visualize decision rules)\n",
    "- No feature scaling required\n",
    "- Fast training and prediction\n",
    "- Handles non-linear relationships naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde40a03-3944-41b6-bdfa-626ef00b505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#            EXPERIMENT 3: DECISION TREES\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 3: DECISION TREES\")\n",
    "println(\"Testing 7 Maximum Depths\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "tree_depths = [3, 5, 7, 10, 15, 20, -1]\n",
    "tree_results = []\n",
    "\n",
    "for (i, max_depth) in enumerate(tree_depths)\n",
    "    depth_str = max_depth == -1 ? \"Unlimited\" : string(max_depth)\n",
    "    println(\"\\n[$i/7] Testing: Depth=$depth_str\")\n",
    "    \n",
    "    hyperparams = Dict(\"max_depth\" => max_depth)\n",
    "    \n",
    "    results = modelCrossValidation(\n",
    "        :DecisionTreeClassifier,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(tree_results, (depth_str, max_depth, f1_stats[1], results))\n",
    "end\n",
    "\n",
    "sorted_tree_results = sort(tree_results, by=x->x[3], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† Decision Tree Results Ranking:\")\n",
    "println(\"-\"^70)\n",
    "for (i, (depth_str, _, f1, _)) in enumerate(sorted_tree_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. Depth=$depth_str - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_desc_tree, best_max_depth_tree, best_f1_tree = sorted_tree_results[1][1:3]\n",
    "println(\"\\nâœ¨ Best Tree: Depth=$best_desc_tree (CV F1: $(round(best_f1_tree*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977cf18-f4e3-4169-b051-671fe2b62ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Decision Tree\n",
    "println(\"\\nðŸš€ Training final Decision Tree on full training set...\")\n",
    "\n",
    "DTClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n",
    "\n",
    "train_inputs_norm_tree = normalizeMinMax(train_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "test_inputs_norm_tree = normalizeMinMax(test_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "\n",
    "train_targets_str_tree = string.(train_targets)\n",
    "test_targets_str_tree = string.(test_targets)\n",
    "\n",
    "if best_max_depth_tree == -1\n",
    "    model_tree = DTClassifier(rng=Random.MersenneTwister(42))\n",
    "else\n",
    "    model_tree = DTClassifier(max_depth=best_max_depth_tree, rng=Random.MersenneTwister(42))\n",
    "end\n",
    "\n",
    "mach_tree = machine(model_tree, MLJ.table(train_inputs_norm_tree), categorical(train_targets_str_tree))\n",
    "MLJ.fit!(mach_tree, verbosity=0)\n",
    "\n",
    "tree_predictions = MLJ.predict(mach_tree, MLJ.table(test_inputs_norm_tree))\n",
    "tree_predictions_mode = mode.(tree_predictions)\n",
    "\n",
    "cm_results_tree = confusionMatrix(tree_predictions_mode, test_targets_str_tree, classes; weighted=true)\n",
    "\n",
    "println(\"\\nðŸ“Š DECISION TREE TEST SET RESULTS:\")\n",
    "println(\"=\"^70)\n",
    "println(\"Accuracy:  $(round(cm_results_tree.accuracy*100, digits=2))%\")\n",
    "println(\"F1 Score:  $(round(cm_results_tree.aggregated.f1*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83fd2b-e43e-43c7-bd54-1f8e3e86951b",
   "metadata": {},
   "source": [
    "# EXPERIMENT 4: k-Nearest Neighbors (kNN)\n",
    "\n",
    "**Configuration:**\n",
    "- **6 k values tested:** 1, 3, 5, 7, 10, 15\n",
    "- **Distance metric:** Euclidean\n",
    "- **Voting:** Majority voting among k neighbors\n",
    "\n",
    "**Notes:**\n",
    "- Feature normalization is CRITICAL for kNN (distance-based)\n",
    "- No explicit training phase (lazy learning)\n",
    "- k=1 is most sensitive to noise\n",
    "- Higher k values create smoother decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbc4c1-ea77-47cf-bf63-7090778cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#            EXPERIMENT 4: k-NEAREST NEIGHBORS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 4: k-NEAREST NEIGHBORS\")\n",
    "println(\"Testing 6 k Values\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "k_values = [1, 3, 5, 7, 10, 15]\n",
    "knn_results = []\n",
    "\n",
    "for (i, k) in enumerate(k_values)\n",
    "    println(\"\\n[$i/6] Testing: k=$k\")\n",
    "    \n",
    "    hyperparams = Dict(\"n_neighbors\" => k)\n",
    "    \n",
    "    results = modelCrossValidation(\n",
    "        :KNeighborsClassifier,\n",
    "        hyperparams,\n",
    "        (train_inputs, train_targets),\n",
    "        cv_indices\n",
    "    )\n",
    "    \n",
    "    acc_stats, err_stats, sens_stats, spec_stats, ppv_stats, npv_stats, f1_stats, cm = results\n",
    "    println(\"    F1: $(round(f1_stats[1]*100, digits=2))% Â± $(round(f1_stats[2]*100, digits=2))%\")\n",
    "    \n",
    "    push!(knn_results, (k, f1_stats[1], results))\n",
    "end\n",
    "\n",
    "sorted_knn_results = sort(knn_results, by=x->x[2], rev=true)\n",
    "\n",
    "println(\"\\nðŸ† kNN Results Ranking:\")\n",
    "println(\"-\"^70)\n",
    "for (i, (k, f1, _)) in enumerate(sorted_knn_results)\n",
    "    badge = i == 1 ? \"ðŸ¥‡\" : i == 2 ? \"ðŸ¥ˆ\" : i == 3 ? \"ðŸ¥‰\" : \"  \"\n",
    "    println(\"$badge $i. k=$k - F1: $(round(f1*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "best_k_knn, best_f1_knn = sorted_knn_results[1][1:2]\n",
    "println(\"\\nâœ¨ Best kNN: k=$best_k_knn (CV F1: $(round(best_f1_knn*100, digits=2))%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3e67c-a3f3-4ddf-8897-e456f64491c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final kNN\n",
    "println(\"\\nðŸš€ Preparing final kNN...\")\n",
    "\n",
    "kNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n",
    "\n",
    "train_inputs_norm_knn = normalizeMinMax(train_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "test_inputs_norm_knn = normalizeMinMax(test_inputs, calculateMinMaxNormalizationParameters(train_inputs))\n",
    "\n",
    "train_targets_str_knn = string.(train_targets)\n",
    "test_targets_str_knn = string.(test_targets)\n",
    "\n",
    "model_knn = kNNClassifier(K=best_k_knn)\n",
    "\n",
    "mach_knn = machine(model_knn, MLJ.table(train_inputs_norm_knn), categorical(train_targets_str_knn))\n",
    "MLJ.fit!(mach_knn, verbosity=0)\n",
    "\n",
    "knn_predictions = MLJ.predict(mach_knn, MLJ.table(test_inputs_norm_knn))\n",
    "knn_predictions_mode = mode.(knn_predictions)\n",
    "\n",
    "cm_results_knn = confusionMatrix(knn_predictions_mode, test_targets_str_knn, classes; weighted=true)\n",
    "\n",
    "println(\"\\nðŸ“Š kNN TEST SET RESULTS:\")\n",
    "println(\"=\"^70)\n",
    "println(\"Accuracy:  $(round(cm_results_knn.accuracy*100, digits=2))%\")\n",
    "println(\"F1 Score:  $(round(cm_results_knn.aggregated.f1*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735d62e-b4f7-4684-91c1-61f8b717a1bd",
   "metadata": {},
   "source": [
    "# EXPERIMENT 5: Ensemble Methods\n",
    "\n",
    "**Strategy:** Combine the top 3 individual models to improve robustness\n",
    "\n",
    "**Models Selected:**\n",
    "1. Best ANN\n",
    "2. Best Decision Tree\n",
    "3. Best kNN\n",
    "\n",
    "**Ensemble Techniques:**\n",
    "1. **Majority Voting:** Each model votes equally, winner takes all\n",
    "2. **Weighted Voting:** Models vote proportionally to their CV F1 scores\n",
    "\n",
    "**Expected Benefits:**\n",
    "- Reduced variance through model averaging\n",
    "- More robust predictions\n",
    "- Leverage complementary strengths of different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a6fc5-c61b-4529-8ac9-44d7266a7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#            EXPERIMENT 5: ENSEMBLE METHODS\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^70)\n",
    "println(\"ðŸ”¬ EXPERIMENT 5: ENSEMBLE METHODS\")\n",
    "println(\"Combining ANN + Decision Tree + kNN\")\n",
    "println(\"=\"^70)\n",
    "\n",
    "# Helper functions for ensemble\n",
    "function majorityVoting(predictions::Vector{Vector{String}})\n",
    "    n_samples = length(predictions[1])\n",
    "    ensemble_predictions = Vector{String}(undef, n_samples)\n",
    "    \n",
    "    for i in 1:n_samples\n",
    "        votes = [pred[i] for pred in predictions]\n",
    "        ensemble_predictions[i] = mode(votes)\n",
    "    end\n",
    "    \n",
    "    return ensemble_predictions\n",
    "end\n",
    "\n",
    "function weightedVoting(predictions::Vector{Vector{String}}, weights::Vector{Float64})\n",
    "    n_samples = length(predictions[1])\n",
    "    n_models = length(predictions)\n",
    "    classes_unique = sort(unique(vcat(predictions...)))\n",
    "    n_classes = length(classes_unique)\n",
    "    \n",
    "    ensemble_predictions = Vector{String}(undef, n_samples)\n",
    "    \n",
    "    for i in 1:n_samples\n",
    "        class_scores = Dict(c => 0.0 for c in classes_unique)\n",
    "        \n",
    "        for j in 1:n_models\n",
    "            class_pred = predictions[j][i]\n",
    "            class_scores[class_pred] += weights[j]\n",
    "        end\n",
    "        \n",
    "        ensemble_predictions[i] = argmax(class_scores)\n",
    "    end\n",
    "    \n",
    "    return ensemble_predictions\n",
    "end\n",
    "\n",
    "# Get test predictions from all 3 models\n",
    "# Convert to string vectors for consistency\n",
    "ann_test_pred_str = string.(argmax.(eachrow(test_outputs_ann)) .- 1)\n",
    "tree_test_pred_str = string.(tree_predictions_mode)\n",
    "knn_test_pred_str = string.(knn_predictions_mode)\n",
    "\n",
    "all_predictions = [ann_test_pred_str, tree_test_pred_str, knn_test_pred_str]\n",
    "\n",
    "# Method 1: Majority Voting\n",
    "println(\"\\n[1/2] Majority Voting...\")\n",
    "majority_predictions = majorityVoting(all_predictions)\n",
    "cm_results_majority = confusionMatrix(majority_predictions, test_targets_str, classes; weighted=true)\n",
    "println(\"âœ… Majority Voting - F1: $(round(cm_results_majority.aggregated.f1*100, digits=2))%\")\n",
    "\n",
    "# Method 2: Weighted Voting\n",
    "println(\"\\n[2/2] Weighted Voting...\")\n",
    "cv_scores = [best_f1_ann, best_f1_tree, best_f1_knn]\n",
    "weights = cv_scores ./ sum(cv_scores)\n",
    "\n",
    "println(\"  Model weights:\")\n",
    "println(\"    ANN:           $(round(weights[1]*100, digits=1))%\")\n",
    "println(\"    Decision Tree: $(round(weights[2]*100, digits=1))%\")\n",
    "println(\"    kNN:           $(round(weights[3]*100, digits=1))%\")\n",
    "\n",
    "weighted_predictions = weightedVoting(all_predictions, weights)\n",
    "cm_results_weighted = confusionMatrix(weighted_predictions, test_targets_str, classes; weighted=true)\n",
    "println(\"âœ… Weighted Voting - F1: $(round(cm_results_weighted.aggregated.f1*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e8a98-7229-4f86-a84e-90297db63ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  APPROACH 3: OVERSAMPLING STRATEGY\n",
    "#  Description: Balance classes by duplicating minority samples instead of removing majority\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"#\"^70)\n",
    "println(\"ðŸ”¬ APPROACH 3: OVERSAMPLING\")\n",
    "println(\"#\"^70)\n",
    "\n",
    "# Function for Random Oversampling\n",
    "function random_oversampling(df, target_col)\n",
    "    classes = unique(df[!, target_col])\n",
    "    # Find count of majority class\n",
    "    max_count = maximum([sum(df[!, target_col] .== c) for c in classes])\n",
    "    \n",
    "    balanced_parts = []\n",
    "    for c in classes\n",
    "        df_class = df[df[!, target_col] .== c, :]\n",
    "        n_current = size(df_class, 1)\n",
    "        if n_current < max_count\n",
    "            # Oversample with replacement\n",
    "            ids = rand(1:n_current, max_count)\n",
    "            push!(balanced_parts, df_class[ids, :])\n",
    "        else\n",
    "            push!(balanced_parts, df_class)\n",
    "        end\n",
    "    end\n",
    "    return vcat(balanced_parts...)\n",
    "end\n",
    "\n",
    "# 1. Prepare Data (Oversampling on Training Data ONLY to prevent leakage)\n",
    "# Note: We use the raw training split created in Approach 1 section\n",
    "df_train_os = random_oversampling(df_train, \"Risk_Class\")\n",
    "\n",
    "# 2. Preprocess (Reuse existing function)\n",
    "df_train_os_proc = preprocess_multiclass(df_train_os, \"Is Fraudulent\")\n",
    "input_cols_os = setdiff(names(df_train_os_proc), [\"Risk_Class\"])\n",
    "\n",
    "train_inputs_os = Matrix{Float64}(df_train_os_proc[:, input_cols_os])\n",
    "train_targets_os = Int.(df_train_os_proc.Risk_Class)\n",
    "\n",
    "# 3. Evaluate ALL models on this new dataset\n",
    "results_app3 = evaluate_approach(\"Oversampling\", train_inputs_os, train_targets_os)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e18970-1c8b-4fc9-b6ba-928fcd592115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  APPROACH 4: FEATURE EXTRACTION (PCA)\n",
    "#  Description: Reduce dimensionality using PCA before modeling\n",
    "# ============================================================================\n",
    "\n",
    "using LinearAlgebra # Required for PCA\n",
    "\n",
    "println(\"\\n\" * \"#\"^70)\n",
    "println(\"ðŸ”¬ APPROACH 4: PCA FEATURE EXTRACTION\")\n",
    "println(\"#\"^70)\n",
    "\n",
    "function apply_pca_transform(data, variance_threshold=0.95)\n",
    "    # 1. Standardize (Zero Mean, Unit Variance)\n",
    "    norm_params = calculateZeroMeanNormalizationParameters(data)\n",
    "    data_std = normalizeZeroMean(data, norm_params)\n",
    "    \n",
    "    # 2. Covariance & Eigen decomposition\n",
    "    C = cov(data_std)\n",
    "    F = eigen(C)\n",
    "    \n",
    "    # Sort eigenvalues desc\n",
    "    idx = sortperm(F.values, rev=true)\n",
    "    evals = F.values[idx]\n",
    "    evecs = F.vectors[:, idx]\n",
    "    \n",
    "    # 3. Select components\n",
    "    cum_var = cumsum(evals ./ sum(evals))\n",
    "    k = findfirst(x -> x >= variance_threshold, cum_var)\n",
    "    println(\"   PCA: Retaining $k components (Variance covered: $(round(cum_var[k]*100, digits=2))%)\")\n",
    "    \n",
    "    # 4. Transform\n",
    "    W = evecs[:, 1:k]\n",
    "    return data_std * W\n",
    "end\n",
    "\n",
    "# 1. Apply PCA to the Standard Undersampled Train Data (from Approach 1)\n",
    "# Using 'train_inputs' from the beginning of the notebook\n",
    "train_inputs_pca = apply_pca_transform(train_inputs, 0.95)\n",
    "\n",
    "# 2. Evaluate ALL models on PCA data\n",
    "results_app4 = evaluate_approach(\"PCA (95% Variance)\", train_inputs_pca, train_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b07d6-cb67-4a8a-a474-c52f02e3e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  APPROACH 5: BINARY CLASSIFICATION (ORIGINAL GROUND TRUTH)\n",
    "#  Description: Train directly on the original \"Is Fraudulent\" label.\n",
    "#  Why: Class 1 (Suspicious) contains both Frauds and Risky Legitimate ones.\n",
    "#       Merging 1 & 2 would confuse the model. We use the true binary label.\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"#\"^70)\n",
    "println(\"ðŸ”¬ APPROACH 5: BINARY CLASSIFICATION (Original Fraud vs Not)\")\n",
    "println(\"#\"^70)\n",
    "\n",
    "# 1. Extract the Original Binary Targets (0/1)\n",
    "# We go back to df_train/df_test because preprocessing dropped the target column\n",
    "println(\"   Extracting original 'Is Fraudulent' labels from balanced dataframe...\")\n",
    "\n",
    "# Ensure we align with the rows used in train_inputs (which come from df_train)\n",
    "train_targets_binary = Int.(df_train[!, \"Is Fraudulent\"])\n",
    "test_targets_binary  = Int.(df_test[!, \"Is Fraudulent\"])\n",
    "\n",
    "# Check distribution\n",
    "n_fraud = sum(train_targets_binary .== 1)\n",
    "n_legit = sum(train_targets_binary .== 0)\n",
    "println(\"   Binary Train Distribution: Legit=$n_legit, Fraud=$n_fraud\")\n",
    "\n",
    "# 2. Evaluate ALL models on Binary Targets\n",
    "# The inputs (train_inputs) remain the same (we keep the feature engineering like \"Is_Night\", etc.)\n",
    "# but we aim for the true binary target.\n",
    "results_app5 = evaluate_approach(\"Binary (Original)\", train_inputs, train_targets_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b598055-3d55-49a8-99bb-d0c01b1f10b2",
   "metadata": {},
   "source": [
    "# Final Results & Comparison\n",
    "\n",
    "Comprehensive comparison of all 6 approaches on the hold-out test set.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- **F1 Score:** Harmonic mean of precision and recall\n",
    "- **Accuracy:** Overall correct predictions\n",
    "- **Per-Class Metrics:** Performance for each risk level\n",
    "\n",
    "**Key Question:** Which approach best balances overall performance with fraud detection capability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900c344-0ae9-401b-bbed-4074445fb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  FINAL COMPARISON SUMMARY - ALL APPROACHES\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"ðŸ† FINAL RESULTS & COMPARISON OF ALL APPROACHES\")\n",
    "println(\"=\"^80)\n",
    "\n",
    "# 1. Recuperiamo i risultati del Primo Approccio (Undersampling) per metterli in tabella\n",
    "# Usiamo le variabili che hai giÃ  calcolato nelle celle precedenti\n",
    "results_base = Dict(\n",
    "    \"ANN\" => cm_results_ann.aggregated.f1,\n",
    "    \"SVM\" => cm_results_svm.aggregated.f1,\n",
    "    \"DT\"  => cm_results_tree.aggregated.f1,\n",
    "    \"kNN\" => cm_results_knn.aggregated.f1\n",
    ")\n",
    "\n",
    "# 2. Funzione per stampare la tabella comparativa\n",
    "using Printf\n",
    "function print_row(name, res)\n",
    "    @printf(\"%-22s | %6.2f%% | %6.2f%% | %6.2f%% | %6.2f%%\\n\", \n",
    "        name, res[\"ANN\"]*100, res[\"SVM\"]*100, res[\"DT\"]*100, res[\"kNN\"]*100)\n",
    "end\n",
    "\n",
    "println(\"Approach               |   ANN    |   SVM    |    DT    |   kNN\")\n",
    "println(\"-\"^75)\n",
    "\n",
    "# Stampa di tutti gli approcci\n",
    "print_row(\"1. Undersampling (Base)\", results_base)\n",
    "print_row(\"2. Oversampling\", results_app3)\n",
    "print_row(\"3. PCA Features\", results_app4)\n",
    "print_row(\"4. Binary Classif.\", results_app5)\n",
    "\n",
    "println(\"-\"^75)\n",
    "\n",
    "# 3. Aggiungiamo i risultati degli Ensemble (che hai fatto solo sull'approccio base)\n",
    "println(\"\\nðŸ“Œ Ensemble Methods (Applied to Base Undersampling):\")\n",
    "println(\"   â€¢ Majority Voting: $(round(cm_results_majority.aggregated.f1*100, digits=2))% (F1 Score)\")\n",
    "println(\"   â€¢ Weighted Voting: $(round(cm_results_weighted.aggregated.f1*100, digits=2))% (F1 Score)\")\n",
    "\n",
    "# 4. Calcolo del vincitore assoluto\n",
    "all_scores = [\n",
    "    (\"Undersampling (Best)\", maximum(values(results_base))),\n",
    "    (\"Oversampling (Best)\", maximum(values(results_app3))),\n",
    "    (\"PCA (Best)\", maximum(values(results_app4))),\n",
    "    (\"Binary (Best)\", maximum(values(results_app5))),\n",
    "    (\"Ensemble Majority\", cm_results_majority.aggregated.f1),\n",
    "    (\"Ensemble Weighted\", cm_results_weighted.aggregated.f1)\n",
    "]\n",
    "\n",
    "best_model = sort(all_scores, by=x->x[2], rev=true)[1]\n",
    "\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"ðŸŽ¯ OVERALL BEST PERFORMANCE: $(best_model[1])\")\n",
    "println(\"   F1 Score: $(round(best_model[2]*100, digits=2))%\")\n",
    "println(\"=\"^80)\n",
    "\n",
    "println(\"\\nðŸ“‹ PROJECT SUMMARY:\")\n",
    "println(\"  âœ… Tested 4 distinct Data Approaches (Under, Over, PCA, Binary)\")\n",
    "println(\"  âœ… Evaluated 4 ML Algorithms (ANN, SVM, DT, kNN) for EACH approach\")\n",
    "println(\"  âœ… Implemented Ensemble methods on the base approach\")\n",
    "println(\"  âœ… Total Experiments: >20 model configurations evaluated\")\n",
    "println(\"=\"^80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612e9c5-1eda-4656-8b0f-a9e12cd61148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  FINAL COMPARISON SUMMARY - ROBUST VERSION\n",
    "# ============================================================================\n",
    "\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"ðŸ† FINAL RESULTS & COMPARISON OF ALL APPROACHES\")\n",
    "println(\"=\"^80)\n",
    "\n",
    "# Funzione helper per recuperare i risultati in modo sicuro\n",
    "function get_f1_safe(var_name)\n",
    "    if isdefined(Main, var_name)\n",
    "        return getfield(Main, var_name).aggregated.f1\n",
    "    else\n",
    "        return -1.0 # Valore sentinella per indicare \"non eseguito\"\n",
    "    end\n",
    "end\n",
    "\n",
    "# 1. Recuperiamo i risultati base (se esistono)\n",
    "f1_ann_base = get_f1_safe(:cm_results_ann)\n",
    "f1_svm_base = get_f1_safe(:cm_results_svm)\n",
    "f1_dt_base  = get_f1_safe(:cm_results_tree)\n",
    "f1_knn_base = get_f1_safe(:cm_results_knn)\n",
    "\n",
    "results_base_defined = (f1_ann_base != -1.0)\n",
    "\n",
    "# 2. Funzione per stampare la tabella\n",
    "using Printf\n",
    "function print_row(name, res_dict)\n",
    "    # Se passiamo un dizionario (nuovi approcci)\n",
    "    ann = res_dict[\"ANN\"] * 100\n",
    "    svm = res_dict[\"SVM\"] * 100\n",
    "    dt  = res_dict[\"DT\"] * 100\n",
    "    knn = res_dict[\"kNN\"] * 100\n",
    "    @printf(\"%-22s | %6.2f%% | %6.2f%% | %6.2f%% | %6.2f%%\\n\", name, ann, svm, dt, knn)\n",
    "end\n",
    "\n",
    "function print_row_base(name, a, s, d, k)\n",
    "    # Se passiamo i valori singoli (approccio base)\n",
    "    if a == -1.0\n",
    "        @printf(\"%-22s |  SKIPPED |  SKIPPED |  SKIPPED |  SKIPPED\\n\", name)\n",
    "    else\n",
    "        @printf(\"%-22s | %6.2f%% | %6.2f%% | %6.2f%% | %6.2f%%\\n\", name, a*100, s*100, d*100, k*100)\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Approach               |   ANN    |   SVM    |    DT    |   kNN\")\n",
    "println(\"-\"^75)\n",
    "\n",
    "# Stampa\n",
    "print_row_base(\"1. Undersampling (Base)\", f1_ann_base, f1_svm_base, f1_dt_base, f1_knn_base)\n",
    "print_row(\"2. Oversampling\", results_app3)\n",
    "print_row(\"3. PCA Features\", results_app4)\n",
    "print_row(\"4. Binary Classif.\", results_app5)\n",
    "\n",
    "println(\"-\"^75)\n",
    "\n",
    "# 3. Ensemble (solo se eseguiti)\n",
    "if isdefined(Main, :cm_results_majority)\n",
    "    println(\"\\nðŸ“Œ Ensemble Methods (Base):\")\n",
    "    println(\"   â€¢ Majority: $(round(cm_results_majority.aggregated.f1*100, digits=2))%\")\n",
    "    println(\"   â€¢ Weighted: $(round(cm_results_weighted.aggregated.f1*100, digits=2))%\")\n",
    "else\n",
    "    println(\"\\nðŸ“Œ Ensemble Methods: SKIPPED (Old experiments not run)\")\n",
    "end\n",
    "\n",
    "# 4. Calcolo Best Overall (escludendo i -1.0)\n",
    "candidates = [\n",
    "    (\"Oversampling\", maximum(values(results_app3))),\n",
    "    (\"PCA\", maximum(values(results_app4))),\n",
    "    (\"Binary\", maximum(values(results_app5)))\n",
    "]\n",
    "\n",
    "if results_base_defined\n",
    "    push!(candidates, (\"Undersampling\", maximum([f1_ann_base, f1_svm_base, f1_dt_base, f1_knn_base])))\n",
    "end\n",
    "\n",
    "best_model = sort(candidates, by=x->x[2], rev=true)[1]\n",
    "\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"ðŸŽ¯ BEST RUNNING APPROACH: $(best_model[1])\")\n",
    "println(\"   F1 Score: $(round(best_model[2]*100, digits=2))%\")\n",
    "println(\"=\"^80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
